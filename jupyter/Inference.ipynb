{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b209c9-86e6-4b4b-9ccc-2f2eab919e8a",
   "metadata": {},
   "source": [
    "# Documenation for backend/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d56943-5b7e-4329-89e3-797f5e1a39cf",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "#### Imports\n",
    "\n",
    "- `os`: Standard library module for operating system interactions.\n",
    "\n",
    "- `create_retrieval_chain`: Function to create a retrieval chain using Langchain.\n",
    "\n",
    "- `create_stuff_documents_chain`: Function to combine multiple documents into a single output.\n",
    "\n",
    "- `ChatMistralAI`: Class for interacting with the Mistral AI model.\n",
    "\n",
    "- Document Loading Functions:\n",
    "\n",
    "    - `load_documents_from_directory`: Loads PDF documents from a directory and splits them into chunks.\n",
    "    - `load_or_create_faiss_vector_store`: Loads or creates a FAISS vector store.\n",
    "    - `get_hybrid_retriever`: Creates a hybrid retriever combining BM25 and vector search.\n",
    "      \n",
    "- `prompt`: Presumably contains predefined prompts for interacting with the AI model.\n",
    "\n",
    "- `get_answer_with_source`: Returns answers with their source references.\n",
    "\n",
    "- `load_dotenv`: Loads environment variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61f065c-cf1e-43e2-b433-d7ad00e0f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized with document path and persist directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Function to load environment variables and initialize paths\n",
    "def initialize_environment():\n",
    "    load_dotenv(override=True)\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")\n",
    "    persist_directory = os.path.join(document_path, \"faiss_indexes\")\n",
    "    print(\"Environment initialized with document path and persist directory.\")\n",
    "    return document_path, persist_directory\n",
    "\n",
    "document_path, persist_directory = initialize_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b6f9c-481f-4615-86c6-d1667662a246",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "- Load environment variables: `load_dotenv(override=True)` Load the environment variables, allowing them to override any existing ones.\n",
    "  \n",
    "- Load Embeddings: Retrieve the document path from the environment variable CORPUS_SOURCE.\n",
    "    `def load_embeddings():\n",
    "    \"\"\"\n",
    "    Load documents and the embeddings from the FAISS vector store.\n",
    "    Returns:\n",
    "        retriever: The hybrid retriever created from the documents and FAISS store.\n",
    "    \"\"\"\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")`\n",
    "\n",
    "  \n",
    "- Raise an error if the document path is not set.\n",
    "\n",
    "   `if not document_path:\n",
    "        raise ValueError(\"CORPUS_SOURCE not found in environment variables.\")`\n",
    "\n",
    "- Define the path for storing the FAISS index.\n",
    "    `persist_directory = os.path.join(document_path, \"faiss_indexes\")`\n",
    "\n",
    "-  Set the number of relevant documents to retrieve.\n",
    "    `top_k = 15` : number of relevant documents to be returned\n",
    "\n",
    "- Load documents\n",
    "    `documents = load_documents_from_directory(document_path)`\n",
    "    - Load documents from the specified directory.\n",
    "\n",
    "-  Print the number of documents loaded for confirmation.\n",
    "`print(f\"Loaded {len(documents)} documents from {document_path}.\")`\n",
    "\n",
    "-  Raise an error if no documents were loaded.\n",
    "\n",
    "        `if not documents:\n",
    "\n",
    "        raise ValueError(\"No documents loaded. Please check the document path.\")`\n",
    "\n",
    "- Create or load FAISS vector store : Create a new FAISS vector store or load an existing one using the loaded documents.\n",
    "    `faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)`\n",
    "  \n",
    "- Get the hybrid retriever: Retrieve a hybrid retriever using the documents and FAISS store\n",
    "    `retriever = get_hybrid_retriever(documents, faiss_store, top_k)`\n",
    "\n",
    "- Print confirmation that embeddings and retriever are ready.\n",
    "\n",
    "  `print(\"Embeddings and retriever loaded.\")`\n",
    "\n",
    "- Return the hybrid retriever for further use: `return retriever`\n",
    "   \n",
    "- Retrieve the Mistral API key from the environment variable.\n",
    "\n",
    "`def get_api_key():\n",
    "    \"\"\"\n",
    "    Get Mistral API Key from the environment variables.\n",
    "    Returns:\n",
    "        str: The Mistral API key.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")`\n",
    "    \n",
    "- Raise an error if the API key is not set  \n",
    "\n",
    "    `if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")`\n",
    "  \n",
    "- Return the retrieved Mistral API key: `return api_key`\n",
    "   \n",
    "- Example usage :  Entry point for the code when run directly.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "- Retrieve the Mistral API key\n",
    "    `try:\n",
    "        retriever = load_embeddings()  # Call to load embeddings\n",
    "        # Load the embeddings and create the retriever.\n",
    "        api_key = get_api_key()        # Call to get API key` \n",
    "        \n",
    "- Print confirmation of successful API key retrieval: `print(f\"Successfully retrieved API Key: {api_key}\")`\n",
    "\n",
    "- Print any error that occurs during the loading process.\n",
    "-    `except ValueError as e:\n",
    "        print(f\"Error: {e}\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a4de34f-ea77-4767-bb70-66aa4f8d51fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from /app/data/swebok...\n",
      "Loaded 470 documents from /app/data/swebok.\n",
      "Loading existing FAISS vector store from /app/data/swebok/faiss_indexes/collection...\n",
      "\n",
      "Embeddings and retriever loaded.\n",
      "Successfully retrieved API Key: KOswaOluwY1jBZqUHPmUGiKIiuR1FubH\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from document_loading import (\n",
    "    load_documents_from_directory, \n",
    "    load_or_create_faiss_vector_store,\n",
    "    get_hybrid_retriever\n",
    ")\n",
    "from prompts import prompt\n",
    "from citations import get_answer_with_source\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"\n",
    "    Load documents and the embeddings from the FAISS vector store.\n",
    "    Returns:\n",
    "        retriever: The hybrid retriever created from the documents and FAISS store.\n",
    "    \"\"\"\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")\n",
    "    if not document_path:\n",
    "        raise ValueError(\"CORPUS_SOURCE not found in environment variables.\")\n",
    "    \n",
    "    persist_directory = os.path.join(document_path, \"faiss_indexes\")\n",
    "    top_k = 15  # number of relevant documents to be returned\n",
    "    \n",
    "    # Load documents\n",
    "    documents = load_documents_from_directory(document_path)\n",
    "    print(f\"Loaded {len(documents)} documents from {document_path}.\")\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents loaded. Please check the document path.\")\n",
    "    \n",
    "    # Create or load FAISS vector store\n",
    "    faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)\n",
    "    \n",
    "    # Get the hybrid retriever\n",
    "    retriever = get_hybrid_retriever(documents, faiss_store, top_k)\n",
    "    \n",
    "    print(\"Embeddings and retriever loaded.\")\n",
    "    return retriever\n",
    "\n",
    "def get_api_key():\n",
    "    \"\"\"\n",
    "    Get Mistral API Key from the environment variables.\n",
    "    Returns:\n",
    "        str: The Mistral API key.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        retriever = load_embeddings()  # Call to load embeddings\n",
    "        api_key = get_api_key()        # Call to get API key\n",
    "        print(f\"Successfully retrieved API Key: {api_key}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb953dae-39b4-46ef-a4ef-7d9c609edf71",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "Function defining:\n",
    "\n",
    "- Defines the function `load_llm_api`, which takes a string argument model_name.\n",
    "  \n",
    "- Docstring: Provides a description of the function's purpose, its arguments, and the return type. The function is designed to load and configure the Mistral AI model.\n",
    "\n",
    "`def load_llm_api(model_name):\n",
    "    \"\"\"\n",
    "    Load and configure the Mistral AI LLM.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "    \n",
    "    Returns:\n",
    "        ChatMistralAI: Configured LLM instance.\n",
    "    \"\"\"\n",
    "`\n",
    "- Retrieve API Key: Uses `os.getenv` to get the Mistral API key from the environment variables.\n",
    "      `api_key = os.getenv(\"MISTRAL_API_KEY\")`\n",
    "\n",
    "- API Key Validation: Checks if the API key was retrieved. If not, it raises a ValueError with a descriptive message, indicating that the API key is missing.\n",
    "\n",
    "    `if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")`\n",
    "\n",
    "- Return Configured LLM Instance: Creates and returns an instance of the ChatMistralAI class, configured with the provided model name, the retrieved API key, and several parameters (temperature, max tokens, and top_p) that control the model's behavior.\n",
    "\n",
    "      ` return ChatMistralAI(\n",
    "        model=model_name,\n",
    "        mistral_api_key=api_key,\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.4,\n",
    "    )`\n",
    "\n",
    "- Define Model Name: Specifies the name of the Mistral AI model to be loaded, in this case, \"open-mistral-7b\".\n",
    "   `MODEL_NAME = \"open-mistral-7b\"`\n",
    "\n",
    "- Attempt to Load Model: Tries to call the `load_llm_api` function with the defined `MODEL_NAME` and assigns the returned instance to the variable llm.\n",
    " \n",
    "`try:\n",
    "    llm = load_llm_api(MODEL_NAME)`\n",
    "    \n",
    "- Success Message: Prints a message indicating that the model has been successfully loaded.\n",
    "     `print(\"Successfully loaded the Mistral LLM.\")`\n",
    "  \n",
    "- Print Model Configuration: These lines print the configuration details of the loaded model, including its name, temperature setting, maximum number of tokens, and top probability value.\n",
    "\n",
    "  ` print(f\"Model Name: {llm.model}\")\n",
    "    print(f\"Temperature: {llm.temperature}\")\n",
    "    print(f\"Max Tokens: {llm.max_tokens}\")\n",
    "    print(f\"Top P: {llm.top_p}\")`\n",
    "\n",
    "- Error Handling: Catches any ValueError raised during the loading process (e.g., if the API key is missing) and prints an error message to inform the user of the issue.\n",
    "\n",
    "  `except ValueError as e:\n",
    "    print(f\"Error: {e}\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b8c715f-70e9-4ac2-80e8-a46c87184907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the Mistral LLM.\n",
      "Model Name: open-mistral-7b\n",
      "Temperature: 0.2\n",
      "Max Tokens: 256\n",
      "Top P: 0.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "def load_llm_api(model_name):\n",
    "    \"\"\"\n",
    "    Load and configure the Mistral AI LLM.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "    \n",
    "    Returns:\n",
    "        ChatMistralAI: Configured LLM instance.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")\n",
    "    \n",
    "    return ChatMistralAI(\n",
    "        model=model_name,\n",
    "        mistral_api_key=api_key,\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.4,\n",
    "    )\n",
    "\n",
    "# Set the model name\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "\n",
    "# Load the model and print its configuration\n",
    "try:\n",
    "    llm = load_llm_api(MODEL_NAME)\n",
    "    print(\"Successfully loaded the Mistral LLM.\")\n",
    "    print(f\"Model Name: {llm.model}\")\n",
    "    print(f\"Temperature: {llm.temperature}\")\n",
    "    print(f\"Max Tokens: {llm.max_tokens}\")\n",
    "    print(f\"Top P: {llm.top_p}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f8732-95d0-4488-bad1-b1470b20e687",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "- Function Declaration: Defines the function `chat_completion_as_dict`, which takes a string parameter question.\n",
    "- Docstring: Provides a description of the function's purpose, its arguments, and the return value.\n",
    "\n",
    "`def chat_completion_as_dict(question):\n",
    "    \"\"\"\n",
    "    Generate a response to a given question using the RAG chain,\n",
    "    returning the answer and context in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the answer and context.\n",
    "    \"\"\"\n",
    "`\n",
    "- Print Statement: Outputs the question to the console, indicating which question is currently being processed.\n",
    "        `print(f\"Running prompt: {question}\")`  \n",
    "  \n",
    "- Question-Answer Chain Creation: Initializes a question-answer chain using the language model `(llm)` and a predefined `prompt`.\n",
    "        `question_answer_chain = create_stuff_documents_chain(llm, prompt)`\n",
    "\n",
    "  \n",
    "- Retrieval Chain Creation: Combines the retriever and the `question_answer_chain` to form a Retrieval-Augmented Generation (RAG) chain.\n",
    "        `rag_chain = create_retrieval_chain(retriever, question_answer_chain)`\n",
    "\n",
    "- Response Initialization: Sets up a dictionary called `full_response` to hold the generated answer and associated context.\n",
    "         `full_response = {\"answer\": \"\", \"context\": []}`\n",
    "\n",
    "- Streaming Loop: Begins iterating over the chunks of data streamed from the rag_chain based on the input question.\n",
    "        `for chunk in rag_chain.stream({\"input\": question}):`  \n",
    "\n",
    "- Answer Check: If the current chunk includes an \"answer\", it appends that answer to `full_response[\"answer\"]`.\n",
    "          ` if \"answer\" in chunk:  \n",
    "            full_response[\"answer\"] += chunk[\"answer\"]` \n",
    "\n",
    "- Context Check: If the chunk contains \"context\", it adds that context to `full_response[\"context\"]`.\n",
    "            `if \"context\" in chunk:  \n",
    "            full_response[\"context\"].extend(chunk[\"context\"])  \n",
    "\n",
    "- Final Answer Extraction: Calls the function get_answer_with_source to process `full_response `and obtain the final answer, potentially with citations\n",
    "    `final_answer = get_answer_with_source(full_response)`\n",
    "\n",
    "- Remaining Answer Calculation: Determines if there is any part of `final_answer` that was not included in the streamed response.\n",
    "          `remaining_answer = final_answer[len(full_response[\"answer\"]):]`\n",
    "\n",
    "- Append Remaining Answer: If there is a `remaining_answer`, it appends this to `full_response[\"answer\"]`.\n",
    "       `if remaining_answer:  \n",
    "        full_response[\"answer\"] += remaining_answer`  \n",
    "\n",
    "  \n",
    "- Return Statement: Constructs and returns a dictionary that includes the complete answer, context, and the model name.\n",
    "\n",
    "  \n",
    "       `return {  # Return a dictionary containing the complete answer, context, and the model name\n",
    "        \"complete_answer\": full_response[\"answer\"], \n",
    "        \"context\": full_response[\"context\"], \n",
    "        \"model\": MODEL_NAME  \n",
    "    }`\n",
    "\n",
    "- Main Check: Ensures that the following code block only runs if the script is executed directly, not if it's imported as a module.\n",
    "     `if __name__ == \"__main__\"`: Check if the script is being run directly\n",
    "\n",
    "- Sample Question Definition: Sets a sample question to test the function.\n",
    "       `question = \"What are the benefits of Retrieval-Augmented Generation?\"`  \n",
    "\n",
    "- Function Call: Executes the `chat_completion_as_dict` function with the sample question.\n",
    "        `response = chat_completion_as_dict(question)`\n",
    "\n",
    "- Print Final Response: Outputs the complete answer, context, and the model name from the returned response to the console.\n",
    "         print(f\"Response: {response['complete_answer']}\\nModel: {response['model']}\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28aa064b-64df-46fc-893a-1c88cf381174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt: What are the benefits of Retrieval-Augmented Generation?\n",
      "Response: Retrieval-Augmented Generation (RAG) is a technique that combines human expertise with machine learning to generate responses to natural language queries. The benefits of RAG include:\n",
      "\n",
      "1. Improved efficiency: RAG can generate responses more quickly than a human alone, as it can process and analyze large amounts of data in a fraction of the time.\n",
      "2. Increased accuracy: By leveraging machine learning algorithms, RAG can reduce the likelihood of errors and improve the overall accuracy of responses.\n",
      "3. Enhanced consistency: RAG can ensure that responses are consistent across different queries, as it can learn from previous interactions and use that knowledge to generate future responses.\n",
      "4. Scalability: RAG can handle a large volume of queries simultaneously, making it an ideal solution for applications with high traffic or complex query structures.\n",
      "5. Cost savings: By automating the response generation process, RAG can reduce the need for human intervention, leading to cost savings in the long run.\n",
      "\n",
      "Overall, RAG can help improve the quality and efficiency of natural language processing applications, making it a valuable tool for developers and organizations looking to enhance their customer service or information retrieval systems.\n",
      "Model: open-mistral-7b\n"
     ]
    }
   ],
   "source": [
    "def chat_completion_as_dict(question):\n",
    "    \"\"\"\n",
    "    Generate a response to a given question using the RAG chain,\n",
    "    returning only the answer in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the answer and model name.\n",
    "    \"\"\"\n",
    "    print(f\"Running prompt: {question}\")  # Print the question being processed\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)  # Create a question-answer chain\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)  # Create a retrieval chain\n",
    "\n",
    "    full_response = {\"answer\": \"\", \"context\": []}  # Initialize the response dictionary\n",
    "\n",
    "    for chunk in rag_chain.stream({\"input\": question}):  # Stream the response from the RAG chain\n",
    "        if \"answer\" in chunk:  # Check if the chunk contains an answer\n",
    "            full_response[\"answer\"] += chunk[\"answer\"]  # Append the answer\n",
    "\n",
    "        if \"context\" in chunk:  # Check if the chunk contains context\n",
    "            full_response[\"context\"].extend(chunk[\"context\"])  # Add context (can be removed)\n",
    "\n",
    "    # You can skip this line to remove sourcing\n",
    "    # final_answer = get_answer_with_source(full_response)  \n",
    "    remaining_answer = full_response[\"answer\"]  # Get the answer without sourcing\n",
    "\n",
    "    # Return the response without sources and context\n",
    "    return {\n",
    "        \"complete_answer\": remaining_answer,  # Return the complete answer\n",
    "        \"model\": MODEL_NAME  # Return the model name\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What are the benefits of Retrieval-Augmented Generation?\"  # Define a sample question\n",
    "    response = chat_completion_as_dict(question)  # Call the function with the question\n",
    "    print(f\"Response: {response['complete_answer']}\\nModel: {response['model']}\")  # Print the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
