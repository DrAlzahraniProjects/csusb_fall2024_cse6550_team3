{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04917ab6-a00d-40c2-8fef-3b0054170c3e",
   "metadata": {},
   "source": [
    "# Documenation for backend/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae0613-433e-46a8-9d1d-8b87c8ccd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from .document_loading import (\n",
    "\tload_documents_from_directory, \n",
    "\tload_or_create_faiss_vector_store,\n",
    "\tget_hybrid_retriever\n",
    ")\n",
    "from .prompts import prompt\n",
    "from .citations import get_answer_with_source\n",
    "\n",
    "# Import and load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d5e9c-43d0-4f9a-9f69-988ea3282240",
   "metadata": {},
   "source": [
    "_Explanation_:\n",
    "\n",
    "__Imports__\n",
    "1. ```os```: Standard library module for operating system interactions.\n",
    "\n",
    "2. ```create_retrieval_chain```: Function to create a retrieval chain using Langchain.\n",
    "\n",
    "3. ```create_stuff_documents_chain```: Function to combine multiple documents into a single output.\n",
    "\n",
    "4. ```ChatMistralAI```: Class for interacting with the Mistral AI model.\n",
    "\n",
    "5. Document Loading Functions:\n",
    "\n",
    "- ```load_documents_from_directory```: Loads PDF documents from a directory and splits them into chunks.\n",
    "- ```load_or_create_faiss_vector_store```: Loads or creates a FAISS vector store.\n",
    "- ```get_hybrid_retriever```: Creates a hybrid retriever combining BM25 and vector search.\n",
    "  \n",
    "6. ```prompt```: Presumably contains predefined prompts for interacting with the AI model.\n",
    "\n",
    "7. ```get_answer_with_source```: Returns answers with their source references.\n",
    "\n",
    "8. ```load_dotenv```: Loads environment variables from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16f045-1ccf-4fb6-ad17-edcce7c861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and the embeddings from the FAISS vector store\n",
    "if os.getenv(\"CORPUS_SOURCE\") == \"\":\n",
    "    document_path = \"data/default/textbook\"\n",
    "    persist_directory = \"data/default/faiss_indexes\"\n",
    "document_path = \"data/default/textbook\"\n",
    "persist_directory = \"data/default/faiss_indexes\"\n",
    "\n",
    "top_k = 15 # number of relevant documents to be returned\n",
    "documents = load_documents_from_directory(document_path)\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, \"pdf_collection\", persist_directory)\n",
    "retriever = get_hybrid_retriever(documents = documents, vector_store = faiss_store, k = top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bea76-72b6-447f-82db-23327169ae71",
   "metadata": {},
   "source": [
    "_Explanation:_\n",
    "\n",
    "1. Check Environment Variable:\n",
    "\n",
    "- Load documents and the embeddings from the FAISS vector store_\n",
    "```\n",
    "if os.getenv(\"CORPUS_SOURCE\") == \"\":\n",
    "    document_path = \"data/default/textbook\"\n",
    "    persist_directory = \"data/default/faiss_indexes\"\n",
    "```\n",
    "\n",
    "- The code checks if the environment variable ```CORPUS_SOURCE``` is empty. \n",
    "- If it is, it sets the ```document_path``` and ```persist_directory``` to default values:\n",
    "```document_path```: ```\"data/default/textbook\"``` - The path where the text documents are located.\n",
    "```persist_directory```: ```\"data/default/faiss_indexes\"``` - The directory where the FAISS vector store will be saved or loaded from.\n",
    "\n",
    "\n",
    "```\n",
    "document_path = \"data/default/textbook\"\n",
    "persist_directory = \"data/default/faiss_indexes\"\n",
    "```\n",
    "\n",
    "\n",
    "2. Set Default Paths:\n",
    "- Regardless of the environment variable, the code reassigns document_path and persist_directory to default values, which may seem redundant if the previous condition is met. This might be intentional for clarity or ensuring consistent behavior.\n",
    "\n",
    "\n",
    "```top_k = 15 # number of relevant documents to be returned```\n",
    "\n",
    "3. Define ```top_k``` :\n",
    "- This variable specifies the number of relevant documents to be returned by the retriever. In this case, it is set to 15.\n",
    "\n",
    "```documents = load_documents_from_directory(document_path)```\n",
    "\n",
    "\n",
    "4. Load Documents:\n",
    "- The ```load_documents_from_directory()``` function is called with the specified ```document_path```. This function loads and splits the documents from the directory into manageable chunks, returning a list of document chunks.\n",
    "\n",
    "```faiss_store = load_or_create_faiss_vector_store(documents, \"pdf_collection\", persist_directory)```\n",
    "\n",
    "5. Load or Create FAISS Vector Store:\n",
    "- The ```load_or_create_faiss_vector_store()``` function is called with the loaded documents, a collection name ```(\"pdf_collection\")```, and the ```persist_directory```.\n",
    "- This function either loads an existing FAISS vector store or creates a new one, returning the vector store object.\n",
    "\n",
    "```retriever = get_hybrid_retriever(documents=documents, vector_store=faiss_store, k=top_k)```\n",
    "\n",
    "6. Create Hybrid Retriever:\n",
    "- The ```get_hybrid_retriever()``` function is called with the loaded documents, the FAISS vector store, and the ```top_k``` value. This function returns an ```EnsembleRetriever``` that combines the BM25 and vector retrieval methods, enabling efficient document searching.\n",
    "\n",
    "_Process Overview_:\n",
    "- This code snippet sets up the environment for document retrieval by loading documents from a directory, creating or loading a FAISS vector store for embeddings, and configuring a hybrid retriever to return a specified number of relevant documents. The use of environment variables allows for flexibility in specifying the document source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed2198-3f6a-494a-a946-ce0af024a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Mistral API Key from the environment variables\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "\traise ValueError(\"MISTRAL_API_KEY not found in .env\")\n",
    "\n",
    "def load_llm_api(model_name):\n",
    "\t\"\"\"\n",
    "\tLoad and configure the Mistral AI LLM.\n",
    "\tReturns:\n",
    "\t\tChatMistralAI: Configured LLM instance.\n",
    "\t\"\"\"\n",
    "\treturn ChatMistralAI(\n",
    "\t\tmodel=model_name,\n",
    "\t\tmistral_api_key=api_key,\n",
    "\t\ttemperature=0.2,\n",
    "\t\tmax_tokens=256,\n",
    "\t\ttop_p=0.4,\n",
    "\t)\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "llm = load_llm_api(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881e323-acd7-4231-ab90-d4c4a73e9a12",
   "metadata": {},
   "source": [
    "_Explanation_:\n",
    "\n",
    "_Get Mistral API Key from the environment variables_\n",
    "\n",
    "```\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"MISTRAL_API_KEY not found in .env\")\n",
    "```\n",
    "\n",
    "1. Retrieve API Key:\n",
    "The code attempts to retrieve the Mistral API key using os.getenv(\"MISTRAL_API_KEY\"). If the key is not found (i.e., it is None), a ValueError is raised with a message indicating that the API key is not present in the .env file.\n",
    "\n",
    "```\n",
    "def load_llm_api(model_name):\n",
    "    \"\"\"\n",
    "    Load and configure the Mistral AI LLM.\n",
    "    Returns:\n",
    "        ChatMistralAI: Configured LLM instance.\n",
    "    \"\"\"\n",
    "```\n",
    "2. Define Function ```load_llm_api``` :\n",
    "This function takes a single parameter, ```model_name```, which specifies the name of the Mistral model to be loaded. It is responsible for creating and returning a configured instance of the ```ChatMistralAI``` class.\n",
    "\n",
    "```\n",
    "return ChatMistralAI(\n",
    "        model=model_name,\n",
    "        mistral_api_key=api_key,\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.4,\n",
    "    )\n",
    "```\n",
    "3. Configure LLM:\n",
    "Inside the function, an instance of ```ChatMistralAI``` is created with the following parameters:\n",
    "- ```model```: The name of the model passed as an argument.\n",
    "- ```mistral_api_key```: The API key retrieved earlier.\n",
    "- ```temperature```: A parameter controlling the randomness of the output (set to 0.2 for relatively deterministic responses).\n",
    "- ```max_tokens```: The maximum number of tokens to generate in the response (set to 256).\n",
    "- ```top_p```: A parameter for nucleus sampling (set to 0.4), determining the diversity of the output.\n",
    "\n",
    "```\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "llm = load_llm_api(MODEL_NAME)\n",
    "```\n",
    "4. Load LLM Instance:\n",
    "The model name ```(\"open-mistral-7b\")``` is assigned to the variable ```MODEL_NAME```. The ```load_llm_api()``` function is then called with ```MODEL_NAME``` to create and configure the LLM instance, which is stored in the variable ```llm```.\n",
    "\n",
    "_Process Overview:_\n",
    "\n",
    "- This code snippet sets up the configuration for using the Mistral AI language model by retrieving the necessary API key from the environment variables, defining a function to load and configure the model, and creating an instance of the LLM for subsequent use. The parameters provided for the model ensure controlled and relevant output generation.\n",
    "\n",
    "- This documentation provides a detailed overview of the code logic, the purpose of each section, and the configuration process for the Mistral AI language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a0c46-a846-4819-86b6-dd6d89294d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(question):\n",
    "  \"\"\"\n",
    "  Generate a response to a given question using the RAG (Retrieval-Augmented Generation) chain,\n",
    "  streaming parts of the response as they are generated.\n",
    "\n",
    "  Args:\n",
    "    question (str): The user question to be answered.\n",
    "\n",
    "  Yields:\n",
    "    str: The generated response in chunks.\n",
    "  \"\"\"\n",
    "  print(f\"Running prompt: {question}\")\n",
    "  question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "  rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "  # Stream response from LLM\n",
    "  full_response = {\"answer\": \"\", \"context\": []}\n",
    "  for chunk in rag_chain.stream({\"input\": question}):\n",
    "    if \"answer\" in chunk:\n",
    "      full_response[\"answer\"] += chunk[\"answer\"]\n",
    "      yield (chunk[\"answer\"], MODEL_NAME)\n",
    "    if \"context\" in chunk:\n",
    "      full_response[\"context\"].extend(chunk[\"context\"])\n",
    "\n",
    "  # After streaming is complete, use the full response to extract citations\n",
    "  final_answer = get_answer_with_source(full_response)\n",
    "  # Yield any remaining part of the answer with citations\n",
    "  remaining_answer = final_answer[len(full_response[\"answer\"]):]\n",
    "  if remaining_answer:\n",
    "    yield (remaining_answer, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e428d-81c1-4b8c-a4b0-a6ded9364bec",
   "metadata": {},
   "source": [
    "_Explanation:_\n",
    "\n",
    "```chat_completion```\n",
    "- This function generates a response to a user-provided question using a retrieval-augmented generation (RAG) chain. It streams parts of the response as they are generated, allowing for a more interactive experience.\n",
    "\n",
    "Parameters\n",
    "```question (str)```:\n",
    "\n",
    "- The user question that needs to be answered.\n",
    "\n",
    "  \n",
    "Yields:\n",
    "- ```str```: The generated response in chunks, allowing the user to receive information incrementally.\n",
    "\n",
    "Function Logic\n",
    "1. Print Question:\n",
    "- The function starts by printing the question being processed to the console for logging purposes:\n",
    "\n",
    "```print(f\"Running prompt: {question}\")```\n",
    "\n",
    "2. Create Chains:\n",
    "\n",
    "It creates two chains using Langchain:\n",
    "- ```question_answer_chain```: This chain is created using ````create_stuff_documents_chain(llm, prompt)```, which combines the LLM and a predefined prompt to answer questions based on document content.\n",
    "- ```rag_chain```: This is created by combining the retriever and the ```question-answer chain``` using ```create_retrieval_chain(retriever, question_answer_chain)```, allowing the function to use both retrieval and generation methods.\n",
    "\n",
    "3. Stream Response:\n",
    "\n",
    "- An initial dictionary, full_response, is created to store the answer and context:\n",
    "\n",
    "```full_response = {\"answer\": \"\", \"context\": []}```\n",
    "\n",
    "\n",
    "- The function then enters a loop that streams the response from the RAG chain:\n",
    "\n",
    "```for chunk in rag_chain.stream({\"input\": question}):```\n",
    "\n",
    "- During this streaming, it checks for ```answer``` and ```context``` keys in each ```chunk```:\n",
    "   - If an ```answer``` is found, it appends it to the ```full_response[\"answer\"]``` and yields the chunk of the answer along with the model name:\n",
    "\n",
    "```yield (chunk[\"answer\"], MODEL_NAME)```\n",
    "\n",
    "- If context is found, it extends the full_response[\"context\"] with the new context data.\n",
    "\n",
    "4. Extract Citations:\n",
    "\n",
    "- After the streaming is complete, the function uses ```get_answer_with_source(full_response)``` to extract citations and construct the final answer. This ensures that the response includes source references for the provided information.\n",
    "\n",
    "5. Yield Remaining Answer:\n",
    "\n",
    "- The function calculates any remaining part of the answer that was not yielded during the streaming process:\n",
    "\n",
    "```remaining_answer = final_answer[len(full_response[\"answer\"]):]```\n",
    "\n",
    "- If there is any remaining answer, it yields this part along with the model name.\n",
    "\n",
    "Process Overview: \n",
    "- The ```chat_completion``` function implements a retrieval-augmented generation mechanism to provide responses to user questions in an interactive manner. It streams answers in chunks and enriches the response with context and citations, enhancing the reliability and traceability of the information generated.\n",
    "\n",
    "- This documentation provides a detailed overview of the function, its parameters, and its logic, ensuring clarity in understanding how the ```chat_completion``` function operates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
