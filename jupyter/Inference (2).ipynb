{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b209c9-86e6-4b4b-9ccc-2f2eab919e8a",
   "metadata": {},
   "source": [
    "# Documenation for backend/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d56943-5b7e-4329-89e3-797f5e1a39cf",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "### Path setup for document retrieval\n",
    "\n",
    "- Environment setup: Loads necessary environment variables for configuration\n",
    "  \n",
    "- File path management: Uses os for handling file paths\n",
    "\n",
    "- Document retrieval: Implements LangChain for document retrieval operations\n",
    "\n",
    "- Combining documents: Creates a chain for synthesizing document content\n",
    "\n",
    "- MistralAI integration: Utilizes ChatMistralAI for chatbot functionalities\n",
    "\n",
    "- Persistence directory: Defines path for storing FAISS index files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61f065c-cf1e-43e2-b433-d7ad00e0f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized with document path and persist directory.\n"
     ]
    }
   ],
   "source": [
    "import os  # Imports the `os` module for interacting with the operating system, crucial for managing file paths in the SWEBOK chatbot.\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain  # Imports the function for creating a retrieval chain, essential for accessing SWEBOK documents.\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Imports document combination functionality, aiding in synthesizing SWEBOK-related information.\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI  # Imports the `ChatMistralAI` class, allowing the chatbot to interact with SWEBOK data effectively.\n",
    "\n",
    "from dotenv import load_dotenv  # Imports `load_dotenv` to load environment variables, enabling flexible configuration for the SWEBOK chatbot.\n",
    "\n",
    "# Function to load environment variables and initialize paths\n",
    "def initialize_environment():\n",
    "    load_dotenv(override=True)  # Loads environment variables from a .env file, allowing customization of paths for SWEBOK resources.\n",
    "\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")  # Retrieves the path to the SWEBOK document corpus from the 'CORPUS_SOURCE' environment variable.\n",
    "\n",
    "    persist_directory = os.path.join(document_path, \"faiss_indexes\")  # Sets the path for storing FAISS index files, essential for efficient document retrieval.\n",
    "\n",
    "    print(\"Environment initialized with document path and persist directory.\")  # Confirms successful setup of paths for the SWEBOK chatbot.\n",
    "\n",
    "    return document_path, persist_directory  # Returns the document path and persist directory for use in SWEBOK content retrieval.\n",
    "\n",
    "# Calls `initialize_environment()` to set up the environment and assigns its output values.\n",
    "document_path, persist_directory = initialize_environment()  \n",
    "# Executes environment initialization to prepare for SWEBOK chatbot operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b6f9c-481f-4615-86c6-d1667662a246",
   "metadata": {},
   "source": [
    "### Environment Setup and Document Retrieval\n",
    "\n",
    "- Imports modules for SWEBOK document retrieval and processing.\n",
    "\n",
    "- Loads environment variables for SWEBOK chatbot configuration.\n",
    "\n",
    "- Loads SWEBOK documents and manages FAISS vector storage.\n",
    "\n",
    "- Creates hybrid retriever for SWEBOK knowledge querying.\n",
    "\n",
    "- Retrieves Mistral API key for chatbot integration.\n",
    "\n",
    "- Handles errors if paths or API keys are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a4de34f-ea77-4767-bb70-66aa4f8d51fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from /app/data/swebok...\n",
      "Loaded 470 documents from /app/data/swebok.\n",
      "Loading existing FAISS vector store from /app/data/swebok/faiss_indexes/collection...\n",
      "\n",
      "Embeddings and retriever loaded.\n",
      "Successfully retrieved API Key: KOswaOluwY1jBZqUHPmUGiKIiuR1FubH\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain.chains.retrieval import create_retrieval_chain \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_mistralai import ChatMistralAI  \n",
    "from document_loading import (  # Imports custom functions to load documents and embeddings.\n",
    "    load_documents_from_directory, \n",
    "    load_or_create_faiss_vector_store,\n",
    "    get_hybrid_retriever\n",
    ")\n",
    "from prompts import prompt  # Imports predefined prompts specific to SWEBOK questions.\n",
    "from citations import get_answer_with_source  # Retrieves SWEBOK answers with source references, supporting reliable responses.\n",
    "from dotenv import load_dotenv  # Loads environment variables, facilitating external configuration for SWEBOK chatbot setup.\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)  # Loads environment variables with possible overrides, setting up SWEBOK chatbot configuration.\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"\n",
    "    Load documents and embeddings from FAISS store.\n",
    "    Returns:\n",
    "        retriever: Hybrid retriever for querying SWEBOK-aligned documents.\n",
    "    \"\"\"\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")  # Retrieves path to SWEBOK corpus from environment settings.\n",
    "    if not document_path:\n",
    "        raise ValueError(\"CORPUS_SOURCE not found in environment variables.\")  # Ensures SWEBOK document source path is specified.\n",
    "    \n",
    "    persist_directory = os.path.join(document_path, \"faiss_indexes\")  # Sets directory path for FAISS index persistence.\n",
    "    top_k = 15  # Specifies number of top relevant documents to retrieve.\n",
    "\n",
    "    # Load SWEBOK-aligned documents\n",
    "    documents = load_documents_from_directory(document_path)  # Loads documents from SWEBOK corpus directory.\n",
    "    print(f\"Loaded {len(documents)} documents from {document_path}.\")  # Confirms document loading for SWEBOK chatbot use.\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents loaded. Please check the document path.\")  # Ensures documents are available for retrieval.\n",
    "\n",
    "    # Create or load FAISS vector store for embeddings\n",
    "    faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)  # Manages embeddings for SWEBOK content retrieval.\n",
    "\n",
    "    # Get the hybrid retriever for querying SWEBOK documents\n",
    "    retriever = get_hybrid_retriever(documents, faiss_store, top_k)  # Sets up retrieval mechanism for SWEBOK queries.\n",
    "\n",
    "    print(\"Embeddings and retriever loaded.\")  # Indicates embeddings and retrieval setup completion for SWEBOK chatbot.\n",
    "    return retriever  # Returns the retriever to support SWEBOK question answering.\n",
    "\n",
    "def get_api_key():\n",
    "    \"\"\"\n",
    "    Get Mistral API Key from environment.\n",
    "    Returns:\n",
    "        str: The API key for SWEBOK chatbot connection.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")  # Retrieves Mistral API key for SWEBOK chatbot interaction.\n",
    "    if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")  # Ensures API key is provided.\n",
    "\n",
    "    return api_key  # Returns API key for SWEBOK chatbot use.\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        retriever = load_embeddings()  # Loads embeddings for SWEBOK content querying.\n",
    "        api_key = get_api_key()        # Retrieves API key for SWEBOK chatbot connection.\n",
    "        print(f\"Successfully retrieved API Key: {api_key}\")  # Confirms API key retrieval.\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")  # Outputs error if loading SWEBOK configuration fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb953dae-39b4-46ef-a4ef-7d9c609edf71",
   "metadata": {},
   "source": [
    "### Mistral AI Setup\n",
    "\n",
    "- Function Usage: Loads and configures the Mistral AI model\n",
    "\n",
    "- API Key Retrieval: Obtains the Mistral API key securely\n",
    "\n",
    "- Model Configuration: Sets parameters like temperature and max tokens\n",
    "\n",
    "- Model Loading Attempt: Loads specified model and confirms successful loading\n",
    "\n",
    "- Error Handling: Catches and displays loading-related exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b8c715f-70e9-4ac2-80e8-a46c87184907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the Mistral LLM.\n",
      "Model Name: open-mistral-7b\n",
      "Temperature: 0.2\n",
      "Max Tokens: 256\n",
      "Top P: 0.4\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "from langchain_mistralai import ChatMistralAI  \n",
    "def load_llm_api(model_name):  # Defines a function that loads and configures the Mistral AI language model (LLM).\n",
    "    \"\"\"\n",
    "    Load and configure the Mistral AI LLM.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "    \n",
    "    Returns:\n",
    "        ChatMistralAI: Configured LLM instance.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")  # Retrieves the Mistral API key from environment variables for authentication.\n",
    "\n",
    "    if not api_key:  # Checks if the API key was successfully retrieved.\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")  # Raises an error if the API key is missing, ensuring that the user is notified.\n",
    "\n",
    "    return ChatMistralAI(  # Creates and returns an instance of the `ChatMistralAI` class with the specified parameters for model configuration.\n",
    "        model=model_name,  # Sets the model name to be used for generating responses.\n",
    "        mistral_api_key=api_key,  # Passes the retrieved API key for authorization when using the Mistral AI service.\n",
    "        temperature=0.2,  # Configures the model's output randomness, with lower values producing more deterministic responses.\n",
    "        max_tokens=256,  # Specifies the maximum number of tokens allowed in the generated output, controlling response length.\n",
    "        top_p=0.4,  # Sets the cumulative probability for token selection, affecting diversity in responses.\n",
    "    )\n",
    "\n",
    "# Set the model name\n",
    "MODEL_NAME = \"open-mistral-7b\"  # Defines the specific model variant to be loaded, which will be utilized in the chatbot.\n",
    "\n",
    "# Load the model and print its configuration\n",
    "try:\n",
    "    llm = load_llm_api(MODEL_NAME)  # Attempts to load the specified model using the previously defined function.\n",
    "    print(\"Successfully loaded the Mistral LLM.\")  # Outputs a confirmation message indicating successful model loading.\n",
    "    print(f\"Model Name: {llm.model}\")  # Prints the name of the loaded model for verification purposes.\n",
    "    print(f\"Temperature: {llm.temperature}\")  # Displays the temperature setting of the model to inform about output randomness.\n",
    "    print(f\"Max Tokens: {llm.max_tokens}\")  # Outputs the maximum number of tokens allowed in the model's responses.\n",
    "    print(f\"Top P: {llm.top_p}\")  # Prints the top-p value to indicate the configuration for token selection.\n",
    "except ValueError as e:  # Catches any ValueError exceptions raised during model loading.\n",
    "    print(f\"Error: {e}\")  # Prints the error message to inform the user about loading failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f8732-95d0-4488-bad1-b1470b20e687",
   "metadata": {},
   "source": [
    "### RAG chain implementation\n",
    "\n",
    "- Function usage: Generates answers using Retrieval-Augmented Generation (RAG) chain\n",
    "\n",
    "- Question logging: Prints the question being processed for debugging\n",
    "\n",
    "- Chain creation: Sets up question-answer and retrieval chains\n",
    "\n",
    "- Response initialization: Initializes a dictionary for storing answers and context\n",
    "\n",
    "- Response streaming: Streams answers and context from the RAG chain\n",
    "\n",
    "- Output structure: Returns complete answer and model name in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28aa064b-64df-46fc-893a-1c88cf381174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prompt: What are the benefits of Retrieval-Augmented Generation?\n",
      "Response: Retrieval-Augmented Generation (RAG) is a technique that combines human expertise with machine learning to generate responses to natural language queries. The benefits of RAG include:\n",
      "\n",
      "1. Improved efficiency: RAG can generate responses more quickly than a human alone, as it can process and analyze large amounts of data in a fraction of the time.\n",
      "2. Increased accuracy: By leveraging machine learning algorithms, RAG can reduce the likelihood of errors and improve the overall accuracy of responses.\n",
      "3. Enhanced consistency: RAG can ensure that responses are consistent across different queries, as it can learn from previous interactions and use that knowledge to generate future responses.\n",
      "4. Scalability: RAG can handle a large volume of queries simultaneously, making it an ideal solution for applications with high traffic or complex query structures.\n",
      "5. Cost savings: By automating the response generation process, RAG can reduce the need for human intervention, leading to cost savings in the long run.\n",
      "\n",
      "Overall, RAG can help improve the quality and efficiency of natural language processing applications, making it a valuable tool for developers and organizations looking to enhance their customer service or information retrieval systems.\n",
      "Model: open-mistral-7b\n"
     ]
    }
   ],
   "source": [
    "# Defines a function that generates a response in dictionary format.\n",
    "def chat_completion_as_dict(question):  \n",
    "    \"\"\"\n",
    "    Generate a response to a given question using the RAG chain,\n",
    "    returning only the answer in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the answer and model name.\n",
    "    \"\"\"\n",
    "    print(f\"Running prompt: {question}\")  # Prints the incoming question for debugging purposes.\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)  # Creates a chain for answering questions using the specified model and prompt.\n",
    "    \n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)  # Creates a retrieval-augmented generation (RAG) chain for enhanced responses.\n",
    "\n",
    "    full_response = {\"answer\": \"\", \"context\": []}  # Initializes a dictionary to hold the final answer and context.\n",
    "\n",
    "    for chunk in rag_chain.stream({\"input\": question}):  # Streams the response from the RAG chain for the provided question.\n",
    "        if \"answer\" in chunk:  # Checks if the current chunk contains an answer.\n",
    "            full_response[\"answer\"] += chunk[\"answer\"]  # Appends the answer to the full_response dictionary.\n",
    "\n",
    "        if \"context\" in chunk:  # Checks if the current chunk contains context information.\n",
    "            full_response[\"context\"].extend(chunk[\"context\"])  # Extends the context list with additional context from the chunk.\n",
    "\n",
    "    \n",
    "    # final_answer = get_answer_with_source(full_response) \n",
    "\n",
    "    remaining_answer = full_response[\"answer\"]  # Extracts the final answer from the full_response dictionary.\n",
    "\n",
    "    # Return the response without sources and context\n",
    "    return {\n",
    "        \"complete_answer\": remaining_answer,  # Returns the complete answer as part of the response.\n",
    "        \"model\": MODEL_NAME  # Returns the name of the model used for generating the answer.\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":  # Ensures the following code runs only if this script is executed directly.\n",
    "    question = \"What are the benefits of Retrieval-Augmented Generation?\"  # Defines a sample question for testing.\n",
    "    \n",
    "    response = chat_completion_as_dict(question)  # Calls the function with the sample question to generate a response.\n",
    "    \n",
    "    print(f\"Response: {response['complete_answer']}\\nModel: {response['model']}\")  # Prints the complete answer and model name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
