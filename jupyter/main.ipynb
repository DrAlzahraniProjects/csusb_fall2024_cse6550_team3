{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot (Team 3)\n",
    "\n",
    "**Purpose**: This chatbot is as an educational tool that's built to answer questions related to the textbook, [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering). The chatbot was built by team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "**Objective**: In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the SWEBOK textbook as the primary data source.\n",
    "\n",
    "**Prerequisites**:\n",
    "Github, Docker, Mamba, Python, Jupyter Notebook\n",
    "\n",
    "**Resources**:\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#1.-Setup)\n",
    "   - [Creating Virtual Environment](#1.1-Creating-Virtual-Environment)\n",
    "   - [Importing Dependencies](#1.2-Importing-dependencies)\n",
    "2. [Building the Chatbot](#2.-Building-the-Chatbot)\n",
    "   - [Document loading](#2.1-Document-loading)\n",
    "   - [Embeddings](#2.2-Embeddings)\n",
    "   - [LLM setup](#2.3-LLM-setup)\n",
    "   - [Mistral loader](#2.4-Mistral-loader)\n",
    "   - [Helpful functions](#2.5-Helpful-functions)\n",
    "3. [Improving the Chatbot with inference](#3.-Improving-the-Chatbot-with-inference)\n",
    "   - [Prompt engineering](#3.1-Prompt-engineering)\n",
    "   - [Helpful functions](#3.2-Other-functions)\n",
    "4. [Testing the Chatbot](#4.-Testing-the-Chatbot)\n",
    "5. [Conclusion](#5.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure compatibility, it is necessary to verify the Python version installed on your system. This project requires Python 3.10 or higher. Follow these steps to check and prepare your environment:\n",
    "\n",
    "Steps to Verify Python Version:\n",
    "\n",
    "- Check Installed Version:\n",
    "Open your terminal or command prompt and execute the following command:\n",
    "\n",
    "- For windows or Linux OS use command\n",
    "    ```python --version```\n",
    "\n",
    "- For Macos Os use command\n",
    "    ```!python3 --version```\n",
    "\n",
    "Dependency Requirements:\n",
    "- Python must already be installed on your system.\n",
    "- Python version of 3.10 or higher is mandatory for this project to function correctly.\n",
    "\n",
    "- If Python is not installed, download and install the latest version of Python from the official Python website.\n",
    "https://www.python.org/downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Virtual Environment\n",
    "This code is setting up a virtual environment for Python. Here's what it does in simple terms:\n",
    "\n",
    "- Install Required Tools:\n",
    "    - It makes sure necessary Python packages (`ipykernel` and `virtualenv`) are installed.\n",
    "    - These are tools needed for creating and managing the virtual environment.\n",
    "\n",
    "- Create a Virtual Environment:\n",
    "    - It creates a virtual environment named `team3_env`.\n",
    "    - A virtual environment is like a separate workspace where you can install Python packages without affecting the global system settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv team3_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!team3_env/bin/pip install ipykernel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec team3_env in /Users/purav/Library/Jupyter/kernels/team3_env\n"
     ]
    }
   ],
   "source": [
    "!team3_env/bin/python3 -m ipykernel install --user --name=team3_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switch Kernel and Verify:\n",
    "\n",
    "1. Switch the Kernel\n",
    "- In the Jupyter Notebook interface, go to the menu bar\n",
    "- Select `Kernel > Change Kernel`.\n",
    "- A list of available kernels will appear.\n",
    "- Switch to ```team3_env```\n",
    "\n",
    "2. Verify the Kernel\n",
    "- After switching the kernel, you can verify it by running the following commands in a new code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using team3_env as kernel!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if \"team3_env\" not in sys.executable:\n",
    "    print(\"Switch kernel to team3_env! You will not be able to proceed unless you switch the kernel to team3_env.\")\n",
    "else:\n",
    "    print(\"Using team3_env as kernel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update `pip` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "print(\"Updating pip\")\n",
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importing dependencies\n",
    "\n",
    "This cell installs essential packages for the chatbot and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies. This can take up to 3 minutes.\n",
      "Upgrading pip...\n",
      "Installing required libraries...\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"\n",
    "    Installs required Python libraries using pip with the -q (quiet) flag.\n",
    "    \"\"\"\n",
    "    print(\"Installing dependencies. This can take up to 3 minutes.\")\n",
    "\n",
    "    # Upgrade pip\n",
    "    try:\n",
    "        print(\"Upgrading pip...\")\n",
    "        !{sys.executable} -m pip install --upgrade pip -q\n",
    "    except Exception as e:\n",
    "        print(f\"Error upgrading pip: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define required libraries\n",
    "    libraries = [\n",
    "        \"faiss-cpu\", \"huggingface_hub\", \"ipykernel\", \"jupyter\", \"langchain\",\n",
    "        \"langchain-community\", \"langchain-huggingface\", \"langchain-mistralai\",\n",
    "        \"python-dotenv\", \"pypdf\", \"requests\", \"sentence-transformers\", \"tiktoken\"\n",
    "    ]\n",
    "\n",
    "    # Install dependencies with -q flag for quiet output\n",
    "    try:\n",
    "        print(\"Installing required libraries...\")\n",
    "        libraries_str = \" \".join(libraries)\n",
    "        !{sys.executable} -m pip install -q {libraries_str}\n",
    "        print(\"Dependencies installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during dependency installation: {e}\")\n",
    "\n",
    "# Call the function to install dependencies\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document loading\n",
    "\n",
    "- **Purpose**: \n",
    "The code loads documents from a specified directory to build the data corpus for the chatbot\n",
    "\n",
    "- **Input**:\n",
    "The input refers to the documents loaded from the directory specified by document_path, which will be used to process user queries related to the chatbot's knowledge base.\n",
    "\n",
    "- **Output**:\n",
    "The output is the collection of documents loaded from the directory into the `documents` variable, which will be used for further processing in the chatbot.\n",
    "\n",
    "- **Processing**:\n",
    "    - The code loads documents from the specified directory into the `documents` variable, creating a corpus for the chatbot to use in responding to queries.\n",
    "    - The primary data source used in this project is [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading faiss_indexes/collection/index.pkl...\n",
      "Successfully downloaded faiss_indexes/collection/index.pkl\n",
      "Downloading faiss_indexes/collection/index.faiss...\n",
      "Successfully downloaded faiss_indexes/collection/index.faiss\n",
      "Downloading textbook.pdf...\n",
      "Successfully downloaded textbook.pdf\n",
      "Download process completed!\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Raw URLs and their local paths\n",
    "files = [\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/49c98e61fc7219c152fb102aa14f630c005150e3/data/swebok/faiss_indexes/collection/index.pkl\",\n",
    "        \"path\": \"faiss_indexes/collection/index.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/49c98e61fc7219c152fb102aa14f630c005150e3/data/swebok/faiss_indexes/collection/index.faiss\",\n",
    "        \"path\": \"faiss_indexes/collection/index.faiss\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/49c98e61fc7219c152fb102aa14f630c005150e3/data/swebok/textbook.pdf\",\n",
    "        \"path\": \"textbook.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('faiss_indexes/collection', exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for file in files:\n",
    "    try:\n",
    "        print(f\"Downloading {file['path']}...\")\n",
    "        response = requests.get(file['url'])\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(file['path'], 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Successfully downloaded {file['path']}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {file['path']}: {e}\")\n",
    "\n",
    "print(\"Download process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document. This can take up to 1 minute.\n",
      "Loading documents from /Users/purav/Documents/CSUSB/CSE_6550_SWE_Concepts/csusb_fall2024_cse6550_team3/jupyter...\n",
      "Number of documents loaded: 412\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Loading document. This can take up to 1 minute.\")\n",
    "\n",
    "# Adds the parent directory to the Python path for module imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) \n",
    "\n",
    "# Defines corpus source and paths for documents and FAISS indexes\n",
    "corpus_source = \"swebok\"  # Sets SWEBOK as corpus\n",
    "document_path = os.getcwd()\n",
    "persist_directory = os.path.join(document_path, \"faiss_indexes\")  # Directory for storing FAISS indexes\n",
    "\n",
    "def load_documents_from_directory(document_path: str, chunk_size: int = 2048, chunk_overlap: int = 200):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a directory and split them into chunks.\n",
    "    \n",
    "    Args:\n",
    "        document_path (str): Path to the directory containing PDF files.\n",
    "        chunk_size (int): Size of each text chunk (default: 2048).\n",
    "        chunk_overlap (int): Overlap between chunks (default: 200).\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading documents from {document_path}...\")\n",
    "\n",
    "        # Check if the document path exists\n",
    "        if not os.path.exists(document_path):\n",
    "            raise FileNotFoundError(f\"Directory not found: {document_path}\")\n",
    "\n",
    "        # Load PDF documents from the specified directory\n",
    "        loader = PyPDFDirectoryLoader(document_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Create a text splitter using tiktoken encoder\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        return text_splitter.split_documents(documents)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load all documents from the defined directory\n",
    "documents = load_documents_from_directory(document_path)\n",
    "\n",
    "# Print the number of documents loaded\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Embeddings\n",
    "\n",
    "- **Purpose**:\n",
    "Download and initialize the embedding model from HuggingFace to generate vector embeddings for text.\n",
    "\n",
    "- **Input**:\n",
    "The input is the model name (`\"Alibaba-NLP/gte-large-en-v1.5\"`) which is used to fetch the embedding model from HuggingFace.\n",
    "\n",
    "- **Output**:\n",
    "The output is the `EMBEDDING_FUNCTION`, which is an instance of the `HuggingFaceEmbeddings` class, ready to generate embeddings for text using the specified model.\n",
    "\n",
    "- **Processing**:\n",
    "    - We have retrieved the textbook, we need to create vector embeddings for it\n",
    "    - We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as our vector database and [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) as our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating/loading the embeddings this will take a couple of minutes...\n",
      "Loaded embedding model successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating/loading the embeddings this will take a couple of minutes...\")\n",
    "# Download the embedding model from HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'trust_remote_code': True})\n",
    "print(\"Loaded embedding model successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Vector store\n",
    "\n",
    "- **Purpose**:\n",
    "To create or load FAISS vector embeddings for the documents, allowing for efficient retrieval during chatbot interactions.\n",
    "\n",
    "- **Input**:\n",
    "The input is the `documents` (loaded documents to be embedded) and `persist_directory` (the directory to store or retrieve the FAISS index).\n",
    "\n",
    "- **Output**:\n",
    "The output is `faiss_store`, which is a FAISS vector store containing the document embeddings for efficient search and retrieval.\n",
    "\n",
    "- **Processing**:\n",
    "    - The `load_or_create_faiss_vector_store` function processes the documents by either creating new FAISS embeddings or loading existing ones from the specified directory (`persist_directory`), enabling fast document retrieval based on vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait... This step can take a long time when performed for the first time\n",
      "Loading existing FAISS vector store from /Users/purav/Documents/CSUSB/CSE_6550_SWE_Concepts/csusb_fall2024_cse6550_team3/jupyter/faiss_indexes/collection...\n",
      "\n",
      "FAISS vector store loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Please wait... This step can take a long time when performed for the first time\")\n",
    "# Using pre-built load_or_create_faiss_vector_store function to create or load FAISS embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def load_or_create_faiss_vector_store(\n",
    "\tdocuments,\n",
    "\tpersist_directory,\n",
    "\tcollection_name=\"collection\"\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad an existing FAISS vector store or create a new one if it doesn't exist.\n",
    "\tArgs:\n",
    "\t\t\tdocuments: List of documents to be indexed.\n",
    "\t\t\tcollection_name (str): Name of the collection.\n",
    "\t\t\tpersist_directory (str): Directory to save/load the FAISS index.\n",
    "\tReturns:\n",
    "\t\t\tFAISS vector store object.\n",
    "\t\"\"\"\n",
    "\tindex_path = os.path.join(persist_directory, f'{collection_name}')\n",
    "\tif os.path.exists(index_path):\n",
    "\t\t# Load existing FAISS index\n",
    "\t\tprint(f\"Loading existing FAISS vector store from {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.load_local(\n",
    "\t\t\tindex_path, \n",
    "\t\t\tembeddings=EMBEDDING_FUNCTION, \n",
    "\t\t\tallow_dangerous_deserialization=True\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\t# Create new FAISS index\n",
    "\t\tprint(f\"Creating new FAISS vector store in {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.from_documents(\n",
    "\t\t\tdocuments, \n",
    "\t\t\tembedding=EMBEDDING_FUNCTION\n",
    "\t\t)\n",
    "\t\tfaiss_store.save_local(index_path)\n",
    "\treturn faiss_store\n",
    "\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)\n",
    "print(\"FAISS vector store loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LLM setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose** - To load environment variables from a `.env` file, retrieve the Mistral API key, and ensure the key is available for further use in the application. If missing, prompt the user to provide the API key via an interactive widget.\n",
    "- **Input** -\n",
    "    - `.env` file containing environment variables (e.g., `MISTRAL_API_KEY)`.\n",
    "    - `api_key`: A string variable that may hold the Mistral API key (manually provided or set via widget).\n",
    "- **Output**\n",
    "    - Success: Prints \"Environment variables successfully set up.\"\n",
    "    - Error: Prompts the user to enter a valid API key using the widget if missing.\n",
    "- **Processing**\n",
    "    - 1. Load Environment Variables: Automatically load the `.env` file if it exists.\n",
    "    - 2. Check for API Key:\n",
    "        - If the key exists, confirm successful setup.\n",
    "        - If the key is missing, display a widget with a text box for input.\n",
    "    - 3. Add API Key Using Widget:\n",
    "Enter the API key in the provided text box.\n",
    "Click the \"Submit\" button to save the key and set it as an environment variable.\n",
    "Feedback: The program provides success or error messages after submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide it using the widget below.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8785e16dfc495293a2c210df582170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='API Key:', layout=Layout(width='400px'), placeholder='Enter your MIâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from ipywidgets import Text, Button, VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Output widget for feedback\n",
    "output = Output()\n",
    "\n",
    "# API key variable\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Function to handle API key input through the widget\n",
    "def create_api_key_widget():\n",
    "    global api_key\n",
    "    api_input = Text(\n",
    "        description=\"API Key:\",\n",
    "        placeholder=\"Enter your MISTRAL API key\",\n",
    "        layout={\"width\": \"400px\"}\n",
    "    )\n",
    "    submit_button = Button(description=\"Submit\", button_style=\"success\")\n",
    "\n",
    "    def on_submit_clicked(_):\n",
    "        global api_key\n",
    "        if api_input.value:\n",
    "            api_key = api_input.value\n",
    "            os.environ[\"MISTRAL_API_KEY\"] = api_key\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                print(\"API key successfully set.\")\n",
    "        else:\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                print(\"Error: Please enter a valid API key.\")\n",
    "\n",
    "    submit_button.on_click(on_submit_clicked)\n",
    "    return VBox([api_input, submit_button, output])\n",
    "\n",
    "# Display the widget only if the API key is missing\n",
    "if not api_key:\n",
    "    print(\"Please provide it using the widget below.\")\n",
    "    api_key_widget = create_api_key_widget()\n",
    "    display(api_key_widget)\n",
    "else:\n",
    "    print(\"Environment variables successfully set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Mistral loader\n",
    "\n",
    "**Purpose**:\n",
    "To load the Mistral AI model (in this case, \"open-mistral-7b\") using the `ChatMistralAI` class from `langchain_mistralai`, and configure it with necessary parameters (such as temperature, max tokens, and top-p) for generating responses.\n",
    "\n",
    "**Input**:\n",
    "- `model_name`: The name of the pre-trained model to be used, here set as \"open-mistral-7b\".\n",
    "- `api_key`: The Mistral API key used for authenticating the model access.\n",
    "\n",
    "**Output**:\n",
    "- The model is loaded and ready to be used for generating responses.\n",
    "- Prints \"Successfully loaded Mistral 7B\" upon successful loading of the model.\n",
    "\n",
    "**Processing**:\n",
    "We will be using [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) as our primary large language model. This will combined with our retriever to create our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Mistral LLM\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load and configure the Mistral AI LLM.\n",
    "MODEL_NAME = \"mistral-large-2411\"\n",
    "llm = ChatMistralAI(model=MODEL_NAME, mistral_api_key=api_key, temperature=0, max_tokens=256)\n",
    "print(\"Successfully loaded Mistral LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity search\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Retrieve and filter the top k most similar documents from the FAISS vector store based on a question.</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li><code>question</code>: The user's query</li>\n",
    "    <li><code>vector_store</code>: FAISS vector store</li>\n",
    "    <li><code>k</code>: Number of similar documents to return</li>\n",
    "    <li><code>distance_threshold</code>: Score threshold for filtering</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Returns a tuple containing filtered relevant documents and their combined content as context string</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "Perform a similarity search in the vector store, filter documents based on the score threshold, return the relevant documents and context</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created similarity search function.\n"
     ]
    }
   ],
   "source": [
    "# Get top k most similar documents using FAISS vector store.\n",
    "def similarity_search(question, vector_store, k, distance_threshold = 400.0):\n",
    "    retrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "    # Filter docs by distance_threshold and process them directly\n",
    "    filtered_docs = [[doc, score] for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "    low_distance_docs = [[doc, score] for doc, score in filtered_docs if score < 320]\n",
    "    \n",
    "    # Select relevant docs based on available results\n",
    "    if low_distance_docs:\n",
    "        relevant_docs = [doc for doc, score in low_distance_docs[:2]]\n",
    "    else:\n",
    "        # If no docs meet the strict criteria, take the first doc from original results\n",
    "        relevant_docs = [doc for doc, _ in filtered_docs[:1]]\n",
    "        \n",
    "    # Clean the text content\n",
    "    for doc in relevant_docs:\n",
    "        doc.page_content = ''.join(char for char in doc.page_content if char.isalpha() or char.isspace() or char in '.,!?\\'\";:()')\n",
    "        \n",
    "    # Create context from cleaned documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    return relevant_docs, context\n",
    "\n",
    "print(\"Created similarity search function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat completion\n",
    "\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Generate responses to a user's question using the RAG system by combining relevant documents and the LLM</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li><code>question</code>: The user's query</li>\n",
    "    <li><code>prompt</code>: The system's prompt format</li>\n",
    "    <li><code>llm</code>: The language model to generate the response</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Yields tuples containing streamed response chunks and their associated source documents</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "Retrieve the top k relevant documents using <code>similarity_search</code>, format the context from the documents and the user query, use the LLM to generate a response, streaming chunks of the answer</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chat completion function.\n"
     ]
    }
   ],
   "source": [
    "# Uses the RAG system to answer the user's questions\n",
    "CORPUS_LINK = f\"SWEBOK (https://www.computer.org/education/bodies-of-knowledge/software-engineering)\"\n",
    "UNANSWERABLE_MSG = f\"I'm a chatbot that only answers questions about {CORPUS_LINK}. Your question appears to be about something else. Could you ask a question related to SWEBOK?\"\n",
    "\n",
    "def chat_completion(question, prompt, llm):\n",
    "    top_k = 2 # The maximum number of documents that similarity search will return\n",
    "    relevant_docs, context  = similarity_search(question, faiss_store, top_k) # Get relevant documents\n",
    "    if not relevant_docs:\n",
    "        yield UNANSWERABLE_MSG, None\n",
    "        return\n",
    "        \n",
    "    messages = prompt.format_messages(input=question, context=context)\n",
    "    # Stream response\n",
    "    full_response = {\"answer\": \"\", \"context\": relevant_docs}\n",
    "    for chunk in llm.stream(messages):\n",
    "        full_response[\"answer\"] += chunk.content\n",
    "        yield (chunk.content, relevant_docs)\n",
    "        \n",
    "print(\"Created chat completion function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prompt engineering\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Define the system behavior for the chatbot and format the prompt template for interacting with the user and providing responses based on the context.</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li>The <code>system_prompt</code> specifies the chatbot's instructions for how to respond</li>\n",
    "    <li>The prompt template defines how the question and context are formatted for the chatbot</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "A formatted prompt template (<code>prompt</code>) that combines the system instructions and user input (question and context) for processing by the LLM</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:\n",
    "<ul>\n",
    "<li>System Behavior Definition: The <code>system_prompt</code> sets rules for how the chatbot should answer questions, handle uncertainty, and identify itself</li>\n",
    "<li>Prompt Template Creation: The <code>ChatPromptTemplate</code> is created with a combination of the system prompt and the user's question/context format, ready to be used for generating the response</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prompt template\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The system prompt will be used as a framework drive the LLM responses\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a chatbot that answers the question inside the <question_start><question_end> tags.\n",
    "Use the context provided to answer the question.\n",
    "If the context is not relavent answer the question if it's related to software engineering.\n",
    "The context is present within the <context_start> and <context_end> tags.\n",
    "\"\"\"\n",
    "\n",
    "# Setting up a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"<question_start>{input}<question_end>\\n\\n<context_start>{context}<context_end>\"),\n",
    "])\n",
    "\n",
    "print(\"Created prompt template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process citations\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Purpose:</strong> Process and format document citations as HTML links</li>\n",
    "    <br>\n",
    "    <li><strong>Input:</strong> docs: List of document objects with metadata</li>\n",
    "    <br>\n",
    "    <li><strong>Output:</strong> str: Formatted HTML citation string with clickable PDF links, or None if no valid citations</li>\n",
    "    <br>\n",
    "    <li><strong>Processing:</strong>\n",
    "        <ul>\n",
    "            <li>Extract page numbers from valid document metadata</li>\n",
    "            <li>Format extracted pages as HTML links pointing to PDF locations</li>\n",
    "            <li>Return formatted citation string or None if no valid citations exist</li>\n",
    "        </ul>\n",
    "    </li>\n",
    " </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created process citations functions.\n"
     ]
    }
   ],
   "source": [
    "def process_citations(docs):\n",
    "    # Extract valid page numbers\n",
    "    citations = [\n",
    "        {'page': doc.metadata.get('page', 'Unknown page') + 1, 'text': ''}\n",
    "        for doc in docs\n",
    "        if isinstance(doc.metadata.get('page'), int)\n",
    "    ][:1]\n",
    "    \n",
    "    if not citations:\n",
    "        return None\n",
    "        \n",
    "    # Format as HTML links using the specific URL format\n",
    "    links = [\n",
    "        f'https://sec.cse.csusb.edu/team3/?view=pdf&file=/app/data/swebok/textbook.pdf&page={citation[\"page\"]}'\n",
    "        for index, citation in enumerate(citations)\n",
    "    ]\n",
    "    return \"\\n\\nSource: \" + \"\".join(links)\n",
    "    \n",
    "print(\"Created process citations functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive chatbot interface\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Provide an interactive interface where the user can input a question (or prompt) and get a response from the chatbot by invoking the RAG system</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:<br>\n",
    "User-provided prompt (query) through the <code>prompt_input</code> widget</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Display the chatbot's response, streamed in chunks, in the output widget</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "<ul>\n",
    "    <li>Create Input/Output Widgets: <code>prompt_input</code> for user input, <code>submit_button</code> for triggering the action, and <code>output</code> to display the response</li>\n",
    "    <li>Button Click Action: When the button is clicked, it triggers the <code>on_submit</code> function</li>\n",
    "    <li>Response Generation: The function uses the <code>chat_completion</code> to generate a response based on the user query. It then streams and displays the response in the <code>output</code> widget</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interactive chatbot interface.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Prompt widget\n",
    "prompt_input = widgets.Text(\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Sumbit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "output = widgets.Output()\n",
    "def on_submit(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        user_prompt = prompt_input.value\n",
    "        if not user_prompt:\n",
    "            user_prompt = \"Who is Hironori Washizaki?\"\n",
    "        print(f\"\\nPrompt: {user_prompt}\\n\")\n",
    "        print(f\"------------------------------\\n\")\n",
    "        # Stream the response\n",
    "        relevant_docs = None\n",
    "        for response_chunk, relevant_docs in chat_completion(user_prompt, prompt, llm):\n",
    "            print(response_chunk, end='', flush=True)\n",
    "            relevant_docs = relevant_docs\n",
    "        if relevant_docs:\n",
    "            print(process_citations(relevant_docs))\n",
    "        \n",
    "submit_button.on_click(on_submit)\n",
    "print(\"Created interactive chatbot interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing the Chatbot\n",
    "\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Render interactive widgets for user input, a submit button, and output display in the notebook</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "<li><code>prompt_input</code>: User's query</li>\n",
    "<li><code>submit_button</code>: Button to trigger response generation</li>\n",
    "<li><code>output</code>: Area to display the response</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Displays input field, submit button, and output area for chatbot interaction</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:\n",
    "<ul>\n",
    "<li>User enters a query and clicks the button</li>\n",
    "<li>The chatbot processes the query and displays the response in the output area</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78bb96f12ac4e8597e713c4d0cebb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', layout=Layout(width='500px'), placeholder='Enter your prompt here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaebf6b0ebb43f4976a9a9c46d65557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b595b22f7c334ec48d222f7410df5b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Recap:\n",
    "- Developed a chatbot using the RAG system, which retrieves relevant documents and generates responses based on the context provided\n",
    "- Integrated widgets in Jupyter Notebook for interactive user input and response display\n",
    "- Configured the chatbot with natural language processing models like Mistral AI and vector-based document retrieval using FAISS\n",
    "\n",
    "Resources:\n",
    "- The Textbook Chatbot project was built by Team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/) offered at CSUSB\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
