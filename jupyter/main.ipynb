{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot (Team 3)\n",
    "\n",
    "**Purpose**: This chatbot is as an educational tool that's built to answer questions related to the textbook, [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering). The chatbot was built by team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "**Objective**: In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the SWEBOK textbook as the primary data source.\n",
    "\n",
    "**Prerequisites**:\n",
    "Github, Docker, Mamba, Python, Jupyter Notebook\n",
    "\n",
    "**Resources**:\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#1.-Setup)\n",
    "   - [Creating Virtual Environment](#1.1-Creating-Virtual-Environment)\n",
    "   - [Importing Dependencies](#1.2-Importing-dependencies)\n",
    "3. [Building the Chatbot](#2.-Building-the-Chatbot)\n",
    "   - [Document loading](#2.1-Document-loading)\n",
    "   - [Embeddings](#2.2-Embeddings)\n",
    "   - [LLM setup](#2.3-LLM-setup)\n",
    "   - [Mistral loader](#2.4-Mistral-loader)\n",
    "4. [Improving the Chatbot with inference](#3.-Improving-the-Chatbot-with-inference)\n",
    "   - [Helpful functions](#3.1-Helpful-functions)\n",
    "   - [Prompt engineering](#3.2-Prompt-engineering)\n",
    "5. [Testing the Chatbot](#4.-Testing-the-Chatbot)\n",
    "6. [Conclusion](#5.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure compatibility, it is necessary to verify the Python version installed on your system. This project requires Python 3.10 or higher. Follow these steps to check and prepare your environment:\n",
    "\n",
    "Steps to Verify Python Version:\n",
    "\n",
    "- Check Installed Version:\n",
    "Open your terminal or command prompt and execute the following command:\n",
    "\n",
    "- For windows or Linux OS use command\n",
    "    ```python --version```\n",
    "\n",
    "- For Macos Os use command\n",
    "    ```!python3 --version```\n",
    "\n",
    "Dependency Requirements:\n",
    "- Python must already be installed on your system.\n",
    "- Python version of 3.10 or higher is mandatory for this project to function correctly.\n",
    "\n",
    "- If Python is not installed, download and install the latest version of Python from the official Python website.\n",
    "https://www.python.org/downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.0\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Virtual Environment\n",
    "This code is setting up a virtual environment for Python. Here's what it does in simple terms:\n",
    "\n",
    "- Install Required Tools:\n",
    "    - It makes sure necessary Python packages (`ipykernel` and `virtualenv`) are installed.\n",
    "    - These are tools needed for creating and managing the virtual environment.\n",
    "\n",
    "- Create a Virtual Environment:\n",
    "    - It creates a virtual environment named `myenv`.\n",
    "    - A virtual environment is like a separate workspace where you can install Python packages without affecting the global system settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv team3_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!team3_env/bin/pip install ipykernel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec team3_env in /Users/purav/Library/Jupyter/kernels/team3_env\n"
     ]
    }
   ],
   "source": [
    "!team3_env/bin/python3 -m ipykernel install --user --name=team3_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switch Kernel and Verify:\n",
    "\n",
    "1. Switch the Kernel\n",
    "- In the Jupyter Notebook interface, go to the menu bar\n",
    "- Select `Kernel > Change Kernel`.\n",
    "- A list of available kernels will appear.\n",
    "- Switch to ```team3_env```\n",
    "\n",
    "2. Verify the Kernel\n",
    "- After switching the kernel, you can verify it by running the following commands in a new code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using team3_env as kernel!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if \"team3_env\" not in sys.executable:\n",
    "    print(\"Switch kernel to team3_env! You will not be able to proceed unless you switch the kernel to team3_env.\")\n",
    "else:\n",
    "    print(\"Using team3_env as kernel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update `pip` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "print(\"Updating pip\")\n",
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importing dependencies\n",
    "\n",
    "This cell installs essential packages for the chatbot and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies. This can take up to 3 minutes.\n",
      "Upgrading pip...\n",
      "Installing required libraries...\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"\n",
    "    Installs required Python libraries using pip with the -q (quiet) flag.\n",
    "    \"\"\"\n",
    "    print(\"Installing dependencies. This can take up to 3 minutes.\")\n",
    "\n",
    "    # Upgrade pip\n",
    "    try:\n",
    "        print(\"Upgrading pip...\")\n",
    "        !{sys.executable} -m pip install --upgrade pip -q\n",
    "    except Exception as e:\n",
    "        print(f\"Error upgrading pip: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define required libraries\n",
    "    libraries = [\n",
    "        \"faiss-cpu\", \"huggingface_hub\", \"ipykernel\", \"jupyter\", \"langchain\",\n",
    "        \"langchain-community\", \"langchain-huggingface\", \"langchain-mistralai\",\n",
    "        \"python-dotenv\", \"pypdf\", \"requests\", \"sentence-transformers\", \"tiktoken\"\n",
    "    ]\n",
    "\n",
    "    # Install dependencies with -q flag for quiet output\n",
    "    try:\n",
    "        print(\"Installing required libraries...\")\n",
    "        libraries_str = \" \".join(libraries)\n",
    "        !{sys.executable} -m pip install -q {libraries_str}\n",
    "        print(\"Dependencies installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during dependency installation: {e}\")\n",
    "\n",
    "# Call the function to install dependencies\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document loading\n",
    "\n",
    "- **Purpose**: \n",
    "The code loads documents from a specified directory to build the data corpus for the chatbot\n",
    "\n",
    "- **Input**:\n",
    "The input refers to the documents loaded from the directory specified by document_path, which will be used to process user queries related to the chatbot's knowledge base.\n",
    "\n",
    "- **Output**:\n",
    "The output is the collection of documents loaded from the directory into the `documents` variable, which will be used for further processing in the chatbot.\n",
    "\n",
    "- **Processing**:\n",
    "    - The code loads documents from the specified directory into the `documents` variable, creating a corpus for the chatbot to use in responding to queries.\n",
    "    - The primary data source used in this project is [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading faiss_indexes/collection/index.pkl...\n",
      "Successfully downloaded faiss_indexes/collection/index.pkl\n",
      "Downloading faiss_indexes/collection/index.faiss...\n",
      "Successfully downloaded faiss_indexes/collection/index.faiss\n",
      "Downloading textbook.pdf...\n",
      "Successfully downloaded textbook.pdf\n",
      "Download process completed!\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Raw URLs and their local paths\n",
    "files = [\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/49c98e61fc7219c152fb102aa14f630c005150e3/data/swebok/faiss_indexes/collection/index.pkl\",\n",
    "        \"path\": \"faiss_indexes/collection/index.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/49c98e61fc7219c152fb102aa14f630c005150e3/data/swebok/faiss_indexes/collection/index.faiss\",\n",
    "        \"path\": \"faiss_indexes/collection/index.faiss\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/49c98e61fc7219c152fb102aa14f630c005150e3/data/swebok/textbook.pdf\",\n",
    "        \"path\": \"textbook.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('faiss_indexes/collection', exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for file in files:\n",
    "    try:\n",
    "        print(f\"Downloading {file['path']}...\")\n",
    "        response = requests.get(file['url'])\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(file['path'], 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Successfully downloaded {file['path']}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {file['path']}: {e}\")\n",
    "\n",
    "print(\"Download process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document. This can take up to 1 minute.\n",
      "Loading documents from /Users/purav/Documents/CSUSB/CSE_6550_SWE_Concepts/csusb_fall2024_cse6550_team3/jupyter...\n",
      "Number of documents loaded: 412\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Loading document. This can take up to 1 minute.\")\n",
    "\n",
    "# Adds the parent directory to the Python path for module imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) \n",
    "\n",
    "# Defines corpus source and paths for documents and FAISS indexes\n",
    "corpus_source = \"swebok\"  # Sets SWEBOK as corpus\n",
    "document_path = os.getcwd()\n",
    "persist_directory = os.path.join(document_path, \"faiss_indexes\")  # Directory for storing FAISS indexes\n",
    "\n",
    "def load_documents_from_directory(document_path: str, chunk_size: int = 2048, chunk_overlap: int = 200):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a directory and split them into chunks.\n",
    "    \n",
    "    Args:\n",
    "        document_path (str): Path to the directory containing PDF files.\n",
    "        chunk_size (int): Size of each text chunk (default: 2048).\n",
    "        chunk_overlap (int): Overlap between chunks (default: 200).\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading documents from {document_path}...\")\n",
    "\n",
    "        # Check if the document path exists\n",
    "        if not os.path.exists(document_path):\n",
    "            raise FileNotFoundError(f\"Directory not found: {document_path}\")\n",
    "\n",
    "        # Load PDF documents from the specified directory\n",
    "        loader = PyPDFDirectoryLoader(document_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Create a text splitter using tiktoken encoder\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        return text_splitter.split_documents(documents)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load all documents from the defined directory\n",
    "documents = load_documents_from_directory(document_path)\n",
    "\n",
    "# Print the number of documents loaded\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Embeddings\n",
    "\n",
    "- **Purpose**:\n",
    "Download and initialize the embedding model from HuggingFace to generate vector embeddings for text.\n",
    "\n",
    "- **Input**:\n",
    "The input is the model name (`\"Alibaba-NLP/gte-large-en-v1.5\"`) which is used to fetch the embedding model from HuggingFace.\n",
    "\n",
    "- **Output**:\n",
    "The output is the `EMBEDDING_FUNCTION`, which is an instance of the `HuggingFaceEmbeddings` class, ready to generate embeddings for text using the specified model.\n",
    "\n",
    "- **Processing**:\n",
    "    - We have retrieved the textbook, we need to create vector embeddings for it\n",
    "    - We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as our vector database and [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) as our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating/loading the embeddings this will take a couple of minutes...\n",
      "Loaded embedding model successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating/loading the embeddings this will take a couple of minutes...\")\n",
    "# Download the embedding model from HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'trust_remote_code': True})\n",
    "print(\"Loaded embedding model successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Vector store\n",
    "\n",
    "- **Purpose**:\n",
    "To create or load FAISS vector embeddings for the documents, allowing for efficient retrieval during chatbot interactions.\n",
    "\n",
    "- **Input**:\n",
    "The input is the `documents` (loaded documents to be embedded) and `persist_directory` (the directory to store or retrieve the FAISS index).\n",
    "\n",
    "- **Output**:\n",
    "The output is `faiss_store`, which is a FAISS vector store containing the document embeddings for efficient search and retrieval.\n",
    "\n",
    "- **Processing**:\n",
    "    - The `load_or_create_faiss_vector_store` function processes the documents by either creating new FAISS embeddings or loading existing ones from the specified directory (`persist_directory`), enabling fast document retrieval based on vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait... This step can take a long time when performed for the first time\n",
      "Loading existing FAISS vector store from /Users/purav/Documents/CSUSB/CSE_6550_SWE_Concepts/csusb_fall2024_cse6550_team3/jupyter/faiss_indexes/collection...\n",
      "\n",
      "FAISS vector store loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Please wait... This step can take a long time when performed for the first time\")\n",
    "# Using pre-built load_or_create_faiss_vector_store function to create or load FAISS embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def load_or_create_faiss_vector_store(\n",
    "\tdocuments,\n",
    "\tpersist_directory,\n",
    "\tcollection_name=\"collection\"\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad an existing FAISS vector store or create a new one if it doesn't exist.\n",
    "\tArgs:\n",
    "\t\t\tdocuments: List of documents to be indexed.\n",
    "\t\t\tcollection_name (str): Name of the collection.\n",
    "\t\t\tpersist_directory (str): Directory to save/load the FAISS index.\n",
    "\tReturns:\n",
    "\t\t\tFAISS vector store object.\n",
    "\t\"\"\"\n",
    "\tindex_path = os.path.join(persist_directory, f'{collection_name}')\n",
    "\tif os.path.exists(index_path):\n",
    "\t\t# Load existing FAISS index\n",
    "\t\tprint(f\"Loading existing FAISS vector store from {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.load_local(\n",
    "\t\t\tindex_path, \n",
    "\t\t\tembeddings=EMBEDDING_FUNCTION, \n",
    "\t\t\tallow_dangerous_deserialization=True\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\t# Create new FAISS index\n",
    "\t\tprint(f\"Creating new FAISS vector store in {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.from_documents(\n",
    "\t\t\tdocuments, \n",
    "\t\t\tembedding=EMBEDDING_FUNCTION\n",
    "\t\t)\n",
    "\t\tfaiss_store.save_local(index_path)\n",
    "\treturn faiss_store\n",
    "\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)\n",
    "print(\"FAISS vector store loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LLM setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "<ul>\n",
    "    <li>To load environment variables from a <code>.env</code> file, retrieve the Mistral API key, and ensure the key is available for further usage in the application.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li><code>.env</code> file containing environment variables (like <code>MISTRAL_API_KEY</code>)</li>\n",
    "    <li><code>api_key</code>: A string variable that may hold the Mistral API key (if manually provided)</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "<ul>\n",
    "    <li>Prints \"Environment variables successfully setup\" if successful, or raises an error if the <code>MISTRAL_API_KEY</code> is not found</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "We have to setup environment variables that will contain our API keys:\n",
    "<ul>\n",
    "    <li>If you have already created a <code>.env</code> file and added the <code>MISTRAL_API_KEY</code> you do not have to do anything</li>\n",
    "    <li>If not, then you can add your API key below. Get an API key <a href=\"https://console.mistral.ai/api-keys/\">here</a></li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables succesfully setup\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = \"\" # add your Mistral API key here if needed\n",
    "if api_key == \"\":\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "elif not api_key:\n",
    "\traise ValueError(\"You have to enter your MISTRAL_API_KEY or add it to a .env file\")\n",
    "print(\"Environment variables succesfully setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Mistral loader\n",
    "\n",
    "**Purpose**:\n",
    "To load the Mistral AI model (in this case, \"open-mistral-7b\") using the `ChatMistralAI` class from `langchain_mistralai`, and configure it with necessary parameters (such as temperature, max tokens, and top-p) for generating responses.\n",
    "\n",
    "**Input**:\n",
    "- `model_name`: The name of the pre-trained model to be used, here set as \"open-mistral-7b\".\n",
    "- `api_key`: The Mistral API key used for authenticating the model access.\n",
    "\n",
    "**Output**:\n",
    "- The model is loaded and ready to be used for generating responses.\n",
    "- Prints \"Successfully loaded Mistral 7B\" upon successful loading of the model.\n",
    "\n",
    "**Processing**:\n",
    "We will be using [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) as our primary large language model. This will combined with our retriever to create our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Mistral LLM\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load and configure the Mistral AI LLM.\n",
    "MODEL_NAME = \"mistral-large-2411\"\n",
    "llm = ChatMistralAI(model=MODEL_NAME, mistral_api_key=api_key, temperature=0, max_tokens=256)\n",
    "print(\"Successfully loaded Mistral LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity search\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Retrieve and filter the top k most similar documents from the FAISS vector store based on a question.</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li><code>question</code>: The user's query</li>\n",
    "    <li><code>vector_store</code>: FAISS vector store</li>\n",
    "    <li><code>k</code>: Number of similar documents to return</li>\n",
    "    <li><code>distance_threshold</code>: Score threshold for filtering</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Filtered list of documents that are most similar to the question</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "Perform a similarity search in the vector store, filter documents based on the score threshold, return the relevant documents</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creared similarity search function.\n"
     ]
    }
   ],
   "source": [
    "# Get top k most similar documents using FAISS vector store.\n",
    "def similarity_search(question, vector_store, k, distance_threshold = 420.0):\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "\treturn filtered_docs\n",
    "print(\"Creared similarity search function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat completion\n",
    "\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Generate responses to a user's question using the RAG system by combining relevant documents and the LLM</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li><code>question</code>: The user's query</li>\n",
    "    <li><code>prompt</code>: The system's prompt format</li>\n",
    "    <li><code>llm</code>: The language model to generate the response</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Streamed response chunks as an answer, enriched with context from the relevant documents</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "Retrieve the top k relevant documents using <code>similarity_search</code>, format the context from the documents and the user query, use the LLM to generate a response, streaming chunks of the answer</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chat completion function.\n"
     ]
    }
   ],
   "source": [
    "# Uses the RAG system to answer the user's questions\n",
    "def chat_completion(question, prompt, llm):\n",
    "    top_k = 10 # The maximum number of documents that similarity search will return\n",
    "    \n",
    "    relevant_docs = similarity_search(question, faiss_store, top_k) # Get relevant documents\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs]) # Format retrived documents\n",
    "    messages = prompt.format_messages(input=question, context=context) \n",
    "    \n",
    "    # Stream response\n",
    "    full_response = {\"answer\": \"\", \"context\": relevant_docs}\n",
    "    for chunk in llm.stream(messages):\n",
    "        full_response[\"answer\"] += chunk.content\n",
    "        yield (chunk.content)\n",
    "print(\"Created chat completion function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive chatbot interface\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Provide an interactive interface where the user can input a question (or prompt) and get a response from the chatbot by invoking the RAG system</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:<br>\n",
    "User-provided prompt (query) through the <code>prompt_input</code> widget</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Display the chatbot's response, streamed in chunks, in the output widget</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:<br>\n",
    "<ul>\n",
    "    <li>Create Input/Output Widgets: <code>prompt_input</code> for user input, <code>submit_button</code> for triggering the action, and <code>output</code> to display the response</li>\n",
    "    <li>Button Click Action: When the button is clicked, it triggers the <code>on_submit</code> function</li>\n",
    "    <li>Response Generation: The function uses the <code>chat_completion</code> to generate a response based on the user query. It then streams and displays the response in the <code>output</code> widget</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interactive chatbot interface.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Prompt widget\n",
    "prompt_input = widgets.Text(\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "# Sumbit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "output = widgets.Output()\n",
    "def on_submit(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        user_prompt = prompt_input.value\n",
    "        if not user_prompt:\n",
    "            user_prompt = \"Who is Hironori Washizaki?\"\n",
    "        print(f\"\\nPrompt: {user_prompt}\\n\")\n",
    "        print(f\"------------------------------\\n\")\n",
    "        # Stream the response\n",
    "        for response_chunk in chat_completion(user_prompt, prompt, llm):\n",
    "            print(response_chunk, end='', flush=True)\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "print(\"Created interactive chatbot interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt engineering\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Define the system behavior for the chatbot and format the prompt template for interacting with the user and providing responses based on the context.</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "    <li>The <code>system_prompt</code> specifies the chatbot's instructions for how to respond</li>\n",
    "    <li>The prompt template defines how the question and context are formatted for the chatbot</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "A formatted prompt template (<code>prompt</code>) that combines the system instructions and user input (question and context) for processing by the LLM</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:\n",
    "<ul>\n",
    "<li>System Behavior Definition: The <code>system_prompt</code> sets rules for how the chatbot should answer questions, handle uncertainty, and identify itself</li>\n",
    "<li>Prompt Template Creation: The <code>ChatPromptTemplate</code> is created with a combination of the system prompt and the user's question/context format, ready to be used for generating the response</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prompt template\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The system prompt will be used as a framework drive the LLM responses\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a chatbot that answers the question inside the <question_start><question_end> tags.\n",
    "Use the context provided to answer the question.\n",
    "If the context is not relavent answer the question if it's related to software engineering.\n",
    "The context is present within the <context_start> and <context_end> tags.\n",
    "\"\"\"\n",
    "\n",
    "# Setting up a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"<question_start>{input}<question_end>\\n\\n<context_start>{context}<context_end>\"),\n",
    "])\n",
    "\n",
    "print(\"Created prompt template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing the Chatbot\n",
    "\n",
    "<ul>\n",
    "<li><strong>Purpose</strong>:<br>\n",
    "Render interactive widgets for user input, a submit button, and output display in the notebook</li>\n",
    "<br>\n",
    "<li><strong>Input</strong>:\n",
    "<ul>\n",
    "<li><code>prompt_input</code>: User's query</li>\n",
    "<li><code>submit_button</code>: Button to trigger response generation</li>\n",
    "<li><code>output</code>: Area to display the response</li>\n",
    "</ul>\n",
    "</li>\n",
    "<br>\n",
    "<li><strong>Output</strong>:<br>\n",
    "Displays input field, submit button, and output area for chatbot interaction</li>\n",
    "<br>\n",
    "<li><strong>Processing</strong>:\n",
    "<ul>\n",
    "<li>User enters a query and clicks the button</li>\n",
    "<li>The chatbot processes the query and displays the response in the output area</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f39dcee38f40d9b4aed3ac5839287d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='MVC', description='Prompt:', layout=Layout(width='500px'), placeholder='Enter your prompt here...'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cb4453abb04b79a8f2d9c137ad6981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5e72db6fc442eca3848bca7797b907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': '\\nPrompt: MVC\\n\\n------------------------------\\n\\nMVC stands for …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Recap:\n",
    "- Developed a chatbot using the RAG system, which retrieves relevant documents and generates responses based on the context provided\n",
    "- Integrated widgets in Jupyter Notebook for interactive user input and response display\n",
    "- Configured the chatbot with natural language processing models like Mistral AI and vector-based document retrieval using FAISS\n",
    "\n",
    "Resources:\n",
    "- The Textbook Chatbot project was built by Team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/) offered at CSUSB\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team3_env",
   "language": "python",
   "name": "team3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
