{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot \n",
    "\n",
    "The Textbook Chatbot project for CSE 6550 is designed to assist with queries related to the textbook.\"Software Engineering: A Practitioner's Approach.\" The chatbot serves as an educational tool, helping users by providing information, answering questions, and possibly retrieving content from the textbook.\n",
    "\n",
    "## Tabel of contents \n",
    "\n",
    "- Setup\n",
    "1. Imports\n",
    "2. Environment variables\n",
    "3. Document loading\n",
    "4. LLM load function\n",
    "\n",
    "- Inference\n",
    "1. chat_completion function\n",
    "2. Prompt template\n",
    "3. User enter's a prompt\n",
    "4. Display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "- This code imports essential libraries for document retrieval, storage, and processing, enabling efficient querying and management of textual data. It uses FAISS and BM25 for high-performance document search, Hugging Face models for embedding text, and tools to split documents and load multiple PDFs from directories.\n",
    "\n",
    "- Environment variables are managed via `dotenv`, making it simple to securely load configuration settings like API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Libraries imported\n",
    "\n",
    "- `os`: For interacting with the operating system and accessing environment variables\n",
    "\n",
    "- `create_retrieval_chain` and `create_stuff_documents_chain` from `langchain.chains`: For building retrieval chains that enable querying and combining relevant documents.\n",
    "\n",
    "- `load_dotenv` from `dotenv`: For loading environment variables from a `.env` file, especially useful for API keys and configuration settings\n",
    "\n",
    "- `FAISS` from `langchain_community.vectorstores`: For creating a FAISS (Facebook AI Similarity Search) vector store, an efficient tool for handling large-scale similarity search\n",
    "\n",
    "- `HuggingFaceEmbeddings` from `langchain_huggingface`: For creating text embeddings using Hugging Face models, which help in encoding textual data into vector form\n",
    "\n",
    "- `BM25Retriever` from `langchain_community.retrievers`: For retrieving documents using BM25, a ranking function commonly used in search engines for information retrieval\n",
    "\n",
    "- `EnsembleRetriever` from `langchain.retrievers`: For combining multiple retrieval methods to improve the accuracy of retrieved results\n",
    "\n",
    "- `RecursiveCharacterTextSplitter` from `langchain_text_splitters`: For splitting large documents into smaller, more manageable chunks\n",
    "\n",
    "- `PyPDFDirectoryLoader` from `langchain_community.document_loaders`: For loading and reading multiple PDF documents within a directory, converting them into text for analysis and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code defines functions for loading and processing documents, specifically PDF files, into manageable chunks, and for creating or loading a FAISS vector store for similarity search. The `load_documents_from_directory function` utilizes a text splitter to break down documents, while `load_or_create_faiss_vector_store` handles the indexing of these chunks.\n",
    "\n",
    "- `similarity_search` and `get_hybrid_retriever` functions facilitate efficient document retrieval using a combination of BM25 and vector-based methods, enhancing the accuracy and relevance of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document loading \n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"  # Embedding model (https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5)\n",
    "model_kwargs = {'trust_remote_code': True}\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs=model_kwargs)\n",
    "\n",
    "def load_documents_from_directory(\n",
    "\tdocument_path: str, \n",
    "\tchunk_size: int = 2048, \n",
    "\tchunk_overlap: int = 200\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad PDF documents from a directory and split them into chunks.\n",
    "\tArgs:\n",
    "\t\tdocument_path (str): Path to the directory containing PDF files.\n",
    "\t\tchunk_size (int): Size of each text chunk (default: 2048).\n",
    "\t\tchunk_overlap (int): Overlap between chunks (default: 200).\n",
    "\tReturns:\n",
    "\t\tList of document chunks.\n",
    "\t\"\"\"\n",
    "\tprint(f\"Loading documents from {document_path}...\")\n",
    "\t# Load PDF documents from the specified directory\n",
    "\tdocuments = PyPDFDirectoryLoader(document_path).load_and_split()\n",
    "\t# Create a text splitter using tiktoken encoder\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\t# Split the documents into chunks\n",
    "\treturn text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def load_or_create_faiss_vector_store(\n",
    "\tdocuments,\n",
    "\tpersist_directory,\n",
    "\tcollection_name=\"collection\"\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad an existing FAISS vector store or create a new one if it doesn't exist.\n",
    "\tArgs:\n",
    "\t\t\tdocuments: List of documents to be indexed.\n",
    "\t\t\tcollection_name (str): Name of the collection.\n",
    "\t\t\tpersist_directory (str): Directory to save/load the FAISS index.\n",
    "\tReturns:\n",
    "\t\t\tFAISS vector store object.\n",
    "\t\"\"\"\n",
    "\tindex_path = os.path.join(persist_directory, f'{collection_name}')\n",
    "\tif os.path.exists(index_path):\n",
    "\t\t# Load existing FAISS index\n",
    "\t\tprint(f\"Loading existing FAISS vector store from {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.load_local(\n",
    "\t\t\tindex_path, \n",
    "\t\t\tembeddings=EMBEDDING_FUNCTION, \n",
    "\t\t\tallow_dangerous_deserialization=True\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\t# Create new FAISS index\n",
    "\t\tprint(f\"Creating new FAISS vector store in {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.from_documents(\n",
    "\t\t\tdocuments, \n",
    "\t\t\tembedding=EMBEDDING_FUNCTION\n",
    "\t\t)\n",
    "\t\tfaiss_store.save_local(index_path)\n",
    "\treturn faiss_store\n",
    "\n",
    "def similarity_search(\n",
    "\tquestion,\n",
    "\tvector_store,\n",
    "\tk,\n",
    "\tdistance_threshold = 420.0\n",
    "):\n",
    "\t\"\"\"\n",
    "\tGet top k most similar documents using FAISS vector store.\n",
    "\tArgs:\n",
    "\t\tquestion: The user question\n",
    "\t\tvector_store: FAISS vector store\n",
    "\t\tk: Number of documents to return\n",
    "\t\tdistance_threshold: Maximum distance score to include document\n",
    "\tReturns:\n",
    "\t\tlist[Document]: Top k most similar documents\n",
    "\t\"\"\"\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "\treturn filtered_docs\n",
    "\n",
    "def get_hybrid_retriever(documents, vector_store, k):\n",
    "\t\"\"\"\n",
    "\tCreate a hybrid retriever combining BM25 and vector search.\n",
    "\tArgs:\n",
    "\t\tdocuments: List of documents for BM25 retriever.\n",
    "\t\tvector_store: FAISS vector store for vector retriever.\n",
    "\t\tk (int): Number of documents to retrieve.\n",
    "\tReturns:\n",
    "\t\tEnsembleRetriever object combining BM25 and vector search.\n",
    "\t\"\"\"\n",
    "\t# Create BM25 retriever\n",
    "\tbm25_retriever = BM25Retriever.from_documents(\n",
    "\t\tdocuments, \n",
    "\t\tk = 0\n",
    "\t)\n",
    "\t# Create vector retriever\n",
    "\tvector_retriever = vector_store.as_retriever(\n",
    "\t\tsearch_type=\"similarity\",\n",
    "\t\tsearch_kwargs={\n",
    "\t\t\t'k': k,\n",
    "\t\t}\n",
    "\t)\n",
    "\t# Combine retrievers with specified weights\n",
    "\tfusion_retriever = EnsembleRetriever(\n",
    "\t\tretrievers=[bm25_retriever, vector_retriever],\n",
    "\t\tweights=[0.2, 0.8]\n",
    "\t)\n",
    "\treturn fusion_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template\n",
    "\n",
    "- This code sets up a chatbot using LangChain that specifically answers questions about the SWEBOK textbook. The `system_prompt` defines guidelines for the chatbot, ensuring it identifies as a chatbot, restricts responses to provided context, avoids fabricating information, and maintains brevity.\n",
    "\n",
    "- The `ChatPromptTemplate` is created from the system instructions and a human input format, allowing for structured interactions. A function retrieves a brief description of the chatbot's purpose, confirming its focus on the SWEBOK content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot prompt for answering textbook-related questions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "# Prompts\n",
    "system_prompt = \"\"\"  # Defines a multi-line string containing system instructions for the chatbot.\n",
    "You are a chatbot answering questions about \"Software Engineering: A Practitioner's Approach\" textbook.  # Specifies the chatbot's context and focus area.\n",
    "\n",
    "1. Always identify yourself as a chatbot, not the textbook.  # Instructs the chatbot to clarify its identity.\n",
    "2. Answer based only on provided context.  # Emphasizes using only the relevant context for responses.\n",
    "3. If unsure, say \"I don't have enough information to answer.\"  # Guides the chatbot on handling uncertainty in answers.\n",
    "4. For unclear questions, ask for clarification.  # Encourages the chatbot to seek more information for ambiguous questions.\n",
    "5. Keep responses under 256 tokens.  # Sets a limit on response length for conciseness.\n",
    "6. Don't invent information.  # Instructs the chatbot to refrain from generating unsupported information.\n",
    "7. Use context only if relevant.  # Advises the chatbot to incorporate context judiciously.\n",
    "8. To questions about your purpose, say: \"I'm a chatbot designed to answer questions about the 'Software Engineering: A Practitioner's Approach' textbook.\"  # Provides a standard response for inquiries about the chatbot's function.\n",
    "\n",
    "Be accurate and concise. Answer only what's asked.  # Reinforces the importance of precision and relevance in responses.\n",
    "\"\"\"\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([  # Creates a chat prompt template from the defined messages.\n",
    "    (\"system\", system_prompt),  # Sets the system prompt as the first message.\n",
    "    (\"human\", \"Question: {input}\\n\\nRelevant Context:\\n{context}\"),  # Defines the human user input format.\n",
    "])\n",
    "\n",
    "def get_chatbot_prompt_description():  # Defines a function that returns a description of the chatbot prompt.\n",
    "    return \"Chatbot prompt for answering textbook-related questions.\"  # Returns a brief description of the chatbot's purpose.\n",
    "\n",
    "# Calls the function to get the prompt description.\n",
    "output = get_chatbot_prompt_description()  \n",
    "print(output)  # Prints the description of the chatbot prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Document Loader\n",
    "\n",
    "- The provided code defines a function `load_documents_from_directory` that loads text documents from a specified directory. It iterates through all files in the given directory and checks if they have a `.txt` extension. For each valid text file, it uses the `TextLoader` from LangChain to load the document's content and appends it to a list.\n",
    "  \n",
    "-   Finally, the function returns a list of all loaded documents, allowing for easy access to the text data for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads text documents from a specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing text documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of loaded documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "            document = loader.load()\n",
    "            documents.extend(document)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Document Embeddings\n",
    "\n",
    "- The `load_embeddings` function retrieves document embeddings from a directory specified by the `CORPUS_SOURCE` environment variable. It checks for the variable's existence, loads the documents, and verifies successful loading. \n",
    "\n",
    "- It then calls `load_or_create_faiss_vector_store` to manage the FAISS vector store and `get_hybrid_retriever` to create a combined retriever. Finally, it confirms the loading process and returns the retriever for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")\n",
    "    if not document_path:\n",
    "        raise ValueError(\"CORPUS_SOURCE not found in environment variables.\")\n",
    "    \n",
    "    persist_directory = os.path.join(document_path, \"faiss_indexes\")\n",
    "    top_k = 15\n",
    "    \n",
    "    # Load documents\n",
    "    documents = load_documents_from_directory(document_path)\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents loaded. Please check the document path.\")\n",
    "\n",
    "    # Assuming `load_or_create_faiss_vector_store` and `get_hybrid_retriever` are defined elsewhere\n",
    "    faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)\n",
    "    retriever = get_hybrid_retriever(documents, faiss_store, top_k)\n",
    "    \n",
    "    print(\"Embeddings and retriever loaded.\")\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral API Loader\n",
    "\n",
    "- The provided code defines a function `load_llm_api` that loads and configures the Mistral AI language model using an API key retrieved from environment variables. If the API key is not set, it raises an error. The function returns an instance of `ChatMistralAI`, configured with specified parameters like model name, temperature, maximum tokens, and top probability.\n",
    "  \n",
    "- The code then initializes the model with the name \"open-mistral-7b.\" A function named `chat_completion_as_dict` is included to interact with the model, taking a question as input and returning the model's response in a structured dictionary format, including the complete answer and the model name.\n",
    "\n",
    "- Retrieve an API key from an `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "def load_llm_api(model_name):\n",
    "    \"\"\"\n",
    "    Load and configure the Mistral AI LLM.\n",
    "    Returns:\n",
    "        ChatMistralAI: Configured LLM instance.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")  # Retrieve API key from environment variable\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is missing. Set the MISTRAL_API_KEY environment variable.\")\n",
    "    \n",
    "    return ChatMistralAI(\n",
    "        model=model_name,\n",
    "        mistral_api_key=api_key,\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.4,\n",
    "    )\n",
    "\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "llm = load_llm_api(MODEL_NAME)\n",
    "\n",
    "# Example function to test the LLM interaction\n",
    "def chat_completion_as_dict(question):\n",
    "    print(f\"Running prompt: {question}\")  # For debugging\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    return {\n",
    "        \"complete_answer\": response.content,\n",
    "        \"model\": MODEL_NAME\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
