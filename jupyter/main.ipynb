{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot (Team 3)\n",
    "\n",
    "This chatbot is as an educational tool that's built to answer questions related to the textbook, [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering). The chatbot was built by team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the SWEBOK textbook as the primary data source.\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)\n",
    "\n",
    "## Table of contents\n",
    "1. [Setup](#1.-Setup)\n",
    "    - 1.1. [Document loading](#1.1-Document-loading)\n",
    "    - 1.2. [Embeddings](#1.2-Embeddings)\n",
    "2. [LLM Setup](#2.-LLM-Setup)\n",
    "    - 2.1. [Environment Variables](#2.1-Environment-Variables)\n",
    "    - 2.2. [Mistral Loader](#2.2-Mistral-Loader)\n",
    "3. [Inference](#3.-Inference)\n",
    "    - 3.1. [Helpful Functions](#3.1-Helpful-Functions)\n",
    "    - 3.2. [Prompt Engineering](#3.2-Prompt-Engineering)\n",
    "    - 3.3. [User Input](#3.2-User-Input) \n",
    "5. [Contributors](#Contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "- This code imports essential libraries for document retrieval, storage, and processing, enabling efficient querying and management of textual data. It uses FAISS and BM25 for high-performance document search, Hugging Face models for embedding text, and tools to split documents and load multiple PDFs from directories.\n",
    "\n",
    "- Environment variables are managed via `dotenv`, making it simple to securely load configuration settings like API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Document loading\n",
    "The primary data source used in this project is [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering).\n",
    "\n",
    "We will begin by setting the `corpus_source` to point to the textbook and processing the textbook PDF\n",
    "\n",
    "This code configures the system path, suppresses warnings, and sets \"swebok\" as the source for the textbook files. It defines paths for the SWEBOK document and FAISS indexes, then loads and processes the textbook PDFs using a backend function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from /app/data/swebok...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Change current directory to root\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "corpus_source = \"swebok\" # Set corpus source\n",
    "\n",
    "# Create a relative path for the textbook\n",
    "document_path = os.path.abspath(os.path.join(\"../data\", corpus_source))\n",
    "persist_directory = os.path.join(document_path, \"faiss_indexes\")\n",
    "\n",
    "# Process textbook PDF\n",
    "from backend.document_loading import load_documents_from_directory\n",
    "documents = load_documents_from_directory(document_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Embeddings\n",
    "Now that we have retrieved the textbook, we need to create vector embeddings for it.\n",
    "\n",
    "`Vector embeddings` are numerical vectors that capture semantic meaning of text. Each chunk of text from our textbook will be converted into a high-dimensional vector that represents its semantic meaning and context. These vectors enable efficient similarity searches and help maintain relationships between related concepts across the text.\n",
    "\n",
    "We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as our vector database and [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) as our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embedding model from HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'trust_remote_code': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the `load_or_create_faiss_vector_store` function to either create or load FAISS embeddings for the processed documents. It initializes faiss_store with these embeddings, storing them in the specified `persist_directory` for efficient text search and retrieval.\n",
    "\n",
    "Creating/loading the embeddings (This will take a couple of minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS vector store from /app/data/swebok/faiss_indexes/collection...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pre-built load_or_create_faiss_vector_store function to create or load FAISS embeddings\n",
    "from backend.document_loading import load_or_create_faiss_vector_store\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to setup environment variables that will contain our API keys.\n",
    "\n",
    "- If you have already created a .env file and added the `MISTRAL_API_KEY` you do not have to do anything. \n",
    "- If not, then you can add your API key below. Get an API key [here](https://console.mistral.ai/api-keys/).\n",
    "\n",
    "This code loads environment variables from a `.env` file with `override=True` to ensure any existing environment variables are overwritten. It checks if an API key is provided in the `api_key` variable; if not, it attempts to retrieve it from the environment variable `MISTRAL_API_KEY`. If neither is available, it raises an error. Finally, it confirms the environment setup with a success message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables succesfully setup\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = \"\" # add your Mistral API key here if needed\n",
    "if api_key == \"\":\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "elif not api_key:\n",
    "\traise ValueError(\"MISTRA API KEY not found\")\n",
    "print(\"Environment variables succesfully setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mistral Loader\n",
    "We will be using [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) as our primary large language model. This will combined with our retriever to create our RAG application.\n",
    "\n",
    "Let's load the LLM using `langchain`\n",
    "\n",
    "This code configures and loads the Mistral AI language model (`open-mistral-7b`) through the `ChatMistralAI` class. It defines a `load_llm` function that sets the model, API key, and other parameters like `temperature`, `max_tokens`, and `top_p` to control response diversity and length. After calling `load_llm` with the specified model name, it initializes `llm` with the configured model and confirms successful loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded Mistral 7B\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load and configure the Mistral AI LLM.\n",
    "model_name = \"open-mistral-7b\"\n",
    "def load_llm(model_name):\n",
    "\treturn ChatMistralAI(\n",
    "\t\tmodel=model_name, # Model name\n",
    "\t\tmistral_api_key=api_key, # Mistral API key\n",
    "\t\ttemperature=0.2,\n",
    "\t\tmax_tokens=256,\n",
    "\t\ttop_p=0.4,\n",
    "\t)\n",
    "    \n",
    "llm = load_llm(model_name)\n",
    "print(\"Succesfully loaded Mistral 7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Helpful Functions\n",
    "Now we will define some helpful functions for RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a `similarity_search` function that retrieves the top k most similar documents related to a given question from a FAISS vector store. It first fetches the k documents with similarity scores, then filters out any documents with scores above a specified `distance_threshold` (defaulting to 420.0). The function returns only the documents that meet the threshold for further processing or analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k most similar documents using FAISS vector store.\n",
    "def similarity_search(question, vector_store, k, distance_threshold = 420.0):\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "\treturn filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a `chat_completion` function that uses a Retrieval-Augmented Generation (RAG) approach to answer user questions. It first retrieves the top 10 most relevant documents from the FAISS vector store using `similarity_search`. The documents are formatted into a single context string, which is added to the prompt for generating a response. The function streams the response from the language model `(llm)` in chunks, building and yielding each part of the answer as it is generated, along with the relevant document context for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the RAG system to answer the user's questions\n",
    "def chat_completion(question, prompt, llm):\n",
    "    top_k = 10 # The maximum number of documents that similarity search will return\n",
    "    \n",
    "    relevant_docs = similarity_search(question, faiss_store, top_k) # Get relevant documents\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs]) # Format retrived documents\n",
    "    messages = prompt.format_messages(input=question, context=context) \n",
    "    \n",
    "    # Stream response\n",
    "    full_response = {\"answer\": \"\", \"context\": relevant_docs}\n",
    "    for chunk in llm.stream(messages):\n",
    "        full_response[\"answer\"] += chunk.content\n",
    "        yield (chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, let's create widgets so the user can ask their question to the RAG system\n",
    "\n",
    "This code sets up a simple interactive interface in Jupyter using `ipywidgets`. It includes a text box `(prompt_input)` for entering a prompt, a submit button `(submit_button)`, and an output area `(output)` for displaying responses.\n",
    "\n",
    "- When the submit button is clicked, the `on_submit` function retrieves the user's prompt from `prompt_input`.\n",
    "- If the input is empty, it defaults to asking \"Who is Hironori Washizaki?\"\n",
    "- The prompt is then passed to the `chat_completion` function, which streams and displays the chatbot's response in real-time within the `output` area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Prompt widget\n",
    "prompt_input = widgets.Text(\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "# Sumbit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "output = widgets.Output()\n",
    "def on_submit(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        user_prompt = prompt_input.value\n",
    "        if not user_prompt:\n",
    "            user_prompt = \"Who is Hironori Washizaki?\"\n",
    "        print(f\"\\nPrompt: {user_prompt}\\n\")\n",
    "        # Stream the response\n",
    "        for response_chunk in chat_completion(user_prompt, prompt, llm):\n",
    "            print(response_chunk, end='', flush=True)\n",
    "\n",
    "submit_button.on_click(on_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Engineering\n",
    "For the LLM to effectively answer our question we have to do some prompt engineering. This will make sure the model stays on track and answers questions with the textbook as the primary context\n",
    "\n",
    "This code creates a system prompt template for guiding the chatbot's responses using `ChatPromptTemplate` from LangChain. The `system_prompt` sets clear guidelines: the chatbot should respond only based on relevant context provided, indicate when it lacks enough information, clarify ambiguous questions, and self-identify as a chatbot. Additionally, it explains its purpose if asked. The `prompt` template uses placeholders for the user question ({`input`}) and the retrieved context ({`context`}) to dynamically insert values when generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The system prompt will be used as a framework drive the LLM responses\n",
    "system_prompt = \"\"\"\n",
    "You are a chatbot that answers the question in the <question> tags.\n",
    "- Answer based only on provided context in <context> tags only if relevant.\n",
    "- If unsure, say \"I don't have enough information to answer.\"\n",
    "- For unclear questions, ask for clarification.\n",
    "- Always identify yourself as a chatbot, not the textbook.\n",
    "- To questions about your purpose, say: \"I'm a chatbot designed to answer questions about the provided textbook.\"\n",
    "\"\"\"\n",
    "\n",
    "# Setting up a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system_prompt),\n",
    "  (\"human\", \"<question>{input}</question>\\n\\n<context>{context}<context>\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 User Input\n",
    "We're done with creating our RAG system!\n",
    "\n",
    "Let's test out the chatbot by asking it a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb53606ea22403bb34eff11c1e5cd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', layout=Layout(width='500px'), placeholder='Enter your prompt here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b805ee1d564104bbf801c4937b5a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9379eb69ea0c46b482d44e04a3a7d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': '\\nPrompt: Who is Hironori Washizaki?\\n\\nHironori Washizaki is an i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "The Textbook Chatbot project was built by Team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/) offered at CSUSB\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
