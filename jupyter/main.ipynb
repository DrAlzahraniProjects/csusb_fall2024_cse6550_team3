{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot (Team 3)\n",
    "\n",
    "This chatbot is as an educational tool that's built to answer questions related to the textbook, [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering). The chatbot was built by team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the SWEBOK textbook as the primary data source.\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)\n",
    "\n",
    "## Table of contents\n",
    "1. [Setup](#1.-Setup)\n",
    "    - 1.1. [Document loading](#1.1-Document-loading)\n",
    "    - 1.2. [Embeddings](#1.2-Embeddings)\n",
    "2. [LLM Setup](#2.-LLM-Setup)\n",
    "    - 2.1. [Environment Variables](#2.1-Environment-Variables)\n",
    "    - 2.2. [Mistral Loader](#2.2-Mistral-Loader)\n",
    "3. [Inference](#3.-Inference)\n",
    "    - 3.1. [Helpful Functions](#3.1-Helpful-Functions)\n",
    "    - 3.2. [Prompt Engineering](#3.2-Prompt-Engineering)\n",
    "    - 3.3. [User Input](#3.2-User-Input) \n",
    "5. [Contributors](#Contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Document loading\n",
    "The primary data source used in this project is [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering).\n",
    "\n",
    "We will begin by setting the `corpus_source` to point to the textbook and processing the textbook PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from /app/data/swebok...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Change current directory to root\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "corpus_source = \"swebok\" # Set corpus source\n",
    "\n",
    "# Create a relative path for the textbook\n",
    "document_path = os.path.abspath(os.path.join(\"../data\", corpus_source))\n",
    "persist_directory = os.path.join(document_path, \"faiss_indexes\")\n",
    "\n",
    "# Process textbook PDF\n",
    "from backend.document_loading import load_documents_from_directory\n",
    "documents = load_documents_from_directory(document_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Embeddings\n",
    "Now that we have retrieved the textbook, we need to create vector embeddings for it.\n",
    "\n",
    "`Vector embeddings` are numerical vectors that capture semantic meaning of text. Each chunk of text from our textbook will be converted into a high-dimensional vector that represents its semantic meaning and context. These vectors enable efficient similarity searches and help maintain relationships between related concepts across the text.\n",
    "\n",
    "We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as our vector database and [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) as our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embedding model from HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'trust_remote_code': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating/loading the embeddings (This will take a couple of minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS vector store from /app/data/swebok/faiss_indexes/collection...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pre-built load_or_create_faiss_vector_store function to create or load FAISS embeddings\n",
    "from backend.document_loading import load_or_create_faiss_vector_store\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to setup environment variables that will contain our API keys.\n",
    "\n",
    "- If you have already created a .env file and added the `MISTRAL_API_KEY` you do not have to do anything. \n",
    "- If not, then you can add your API key below. Get an API key [here](https://console.mistral.ai/api-keys/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables succesfully setup\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = \"\" # add your Mistral API key here if needed\n",
    "if api_key == \"\":\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "elif not api_key:\n",
    "\traise ValueError(\"MISTRA API KEY not found\")\n",
    "print(\"Environment variables succesfully setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mistral Loader\n",
    "We will be using [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) as our primary large language model. This will combined wiht our retriever to create our RAG application.\n",
    "\n",
    "Let's load the LLM using `langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded Mistral 7B\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load and configure the Mistral AI LLM.\n",
    "model_name = \"open-mistral-7b\"\n",
    "def load_llm(model_name):\n",
    "\treturn ChatMistralAI(\n",
    "\t\tmodel=model_name, # Model name\n",
    "\t\tmistral_api_key=api_key, # Mistral API key\n",
    "\t\ttemperature=0.2,\n",
    "\t\tmax_tokens=256,\n",
    "\t\ttop_p=0.4,\n",
    "\t)\n",
    "    \n",
    "llm = load_llm(model_name)\n",
    "print(\"Succesfully loaded Mistral 7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Helpful Functions\n",
    "Now we will define some helpful functions for RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the `similarity_search` function for similarity search\n",
    "- Similarity search will be used in combination with FAISS embeddings to find relevant documents that can be used by our LLM. \n",
    "- We're using a distance threshold to filter out irrelavent documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k most similar documents using FAISS vector store.\n",
    "def similarity_search(question, vector_store, k, distance_threshold = 420.0):\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "\treturn filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's define the `chat_completion` function that will serve as the primary way to interact with our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the RAG system to answer the user's questions\n",
    "def chat_completion(question, prompt, llm):\n",
    "    top_k = 10 # The maximum number of documents that similarity search will return\n",
    "    \n",
    "    relevant_docs = similarity_search(question, faiss_store, top_k) # Get relevant documents\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs]) # Format retrived documents\n",
    "    messages = prompt.format_messages(input=question, context=context) \n",
    "    \n",
    "    # Stream response\n",
    "    full_response = {\"answer\": \"\", \"context\": relevant_docs}\n",
    "    for chunk in llm.stream(messages):\n",
    "        full_response[\"answer\"] += chunk.content\n",
    "        yield (chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, let's create widgets so the user can ask their question to the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Prompt widget\n",
    "prompt_input = widgets.Text(\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "# Sumbit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "output = widgets.Output()\n",
    "def on_submit(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        user_prompt = prompt_input.value\n",
    "        if not user_prompt:\n",
    "            user_prompt = \"Who is Hironori Washizaki?\"\n",
    "        print(f\"\\nPrompt: {user_prompt}\\n\")\n",
    "        # Stream the response\n",
    "        for response_chunk in chat_completion(user_prompt, prompt, llm):\n",
    "            print(response_chunk, end='', flush=True)\n",
    "\n",
    "submit_button.on_click(on_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Engineering\n",
    "For the LLM to effectively answer our question we have to do some prompt engineering. This will make sure the model stays on track and answers questions with the textbook as the primary context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The system prompt will be used as a framework drive the LLM responses\n",
    "system_prompt = \"\"\"\n",
    "You are a chatbot that answers the question in the <question> tags.\n",
    "- Answer based only on provided context in <context> tags only if relevant.\n",
    "- If unsure, say \"I don't have enough information to answer.\"\n",
    "- For unclear questions, ask for clarification.\n",
    "- Always identify yourself as a chatbot, not the textbook.\n",
    "- To questions about your purpose, say: \"I'm a chatbot designed to answer questions about the provided textbook.\"\n",
    "\"\"\"\n",
    "\n",
    "# Setting up a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system_prompt),\n",
    "  (\"human\", \"<question>{input}</question>\\n\\n<context>{context}<context>\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 User Input\n",
    "We're done with creating our RAG system!\n",
    "\n",
    "Let's test out the chatbot by asking it a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb53606ea22403bb34eff11c1e5cd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', layout=Layout(width='500px'), placeholder='Enter your prompt here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b805ee1d564104bbf801c4937b5a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9379eb69ea0c46b482d44e04a3a7d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': '\\nPrompt: Who is Hironori Washizaki?\\n\\nHironori Washizaki is an i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "The Textbook Chatbot project was built by Team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/) offered at CSUSB\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "- This code imports essential libraries for document retrieval, storage, and processing, enabling efficient querying and management of textual data. It uses FAISS and BM25 for high-performance document search, Hugging Face models for embedding text, and tools to split documents and load multiple PDFs from directories.\n",
    "\n",
    "- Environment variables are managed via `dotenv`, making it simple to securely load configuration settings like API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Libraries imported\n",
    "\n",
    "- `os`: For interacting with the operating system and accessing environment variables\n",
    "\n",
    "- `create_retrieval_chain` and `create_stuff_documents_chain` from `langchain.chains`: For building retrieval chains that enable querying and combining relevant documents.\n",
    "\n",
    "- `load_dotenv` from `dotenv`: For loading environment variables from a `.env` file, especially useful for API keys and configuration settings\n",
    "\n",
    "- `FAISS` from `langchain_community.vectorstores`: For creating a FAISS (Facebook AI Similarity Search) vector store, an efficient tool for handling large-scale similarity search\n",
    "\n",
    "- `HuggingFaceEmbeddings` from `langchain_huggingface`: For creating text embeddings using Hugging Face models, which help in encoding textual data into vector form\n",
    "\n",
    "- `BM25Retriever` from `langchain_community.retrievers`: For retrieving documents using BM25, a ranking function commonly used in search engines for information retrieval\n",
    "\n",
    "- `EnsembleRetriever` from `langchain.retrievers`: For combining multiple retrieval methods to improve the accuracy of retrieved results\n",
    "\n",
    "- `RecursiveCharacterTextSplitter` from `langchain_text_splitters`: For splitting large documents into smaller, more manageable chunks\n",
    "\n",
    "- `PyPDFDirectoryLoader` from `langchain_community.document_loaders`: For loading and reading multiple PDF documents within a directory, converting them into text for analysis and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code defines functions for loading and processing documents, specifically PDF files, into manageable chunks, and for creating or loading a FAISS vector store for similarity search. The `load_documents_from_directory function` utilizes a text splitter to break down documents, while `load_or_create_faiss_vector_store` handles the indexing of these chunks.\n",
    "\n",
    "- `similarity_search` and `get_hybrid_retriever` functions facilitate efficient document retrieval using a combination of BM25 and vector-based methods, enhancing the accuracy and relevance of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document loading \n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"  # Embedding model (https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5)\n",
    "model_kwargs = {'trust_remote_code': True}\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs=model_kwargs)\n",
    "\n",
    "def load_documents_from_directory(\n",
    "\tdocument_path: str, \n",
    "\tchunk_size: int = 2048, \n",
    "\tchunk_overlap: int = 200\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad PDF documents from a directory and split them into chunks.\n",
    "\tArgs:\n",
    "\t\tdocument_path (str): Path to the directory containing PDF files.\n",
    "\t\tchunk_size (int): Size of each text chunk (default: 2048).\n",
    "\t\tchunk_overlap (int): Overlap between chunks (default: 200).\n",
    "\tReturns:\n",
    "\t\tList of document chunks.\n",
    "\t\"\"\"\n",
    "\tprint(f\"Loading documents from {document_path}...\")\n",
    "\t# Load PDF documents from the specified directory\n",
    "\tdocuments = PyPDFDirectoryLoader(document_path).load_and_split()\n",
    "\t# Create a text splitter using tiktoken encoder\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\t# Split the documents into chunks\n",
    "\treturn text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def load_or_create_faiss_vector_store(\n",
    "\tdocuments,\n",
    "\tpersist_directory,\n",
    "\tcollection_name=\"collection\"\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad an existing FAISS vector store or create a new one if it doesn't exist.\n",
    "\tArgs:\n",
    "\t\t\tdocuments: List of documents to be indexed.\n",
    "\t\t\tcollection_name (str): Name of the collection.\n",
    "\t\t\tpersist_directory (str): Directory to save/load the FAISS index.\n",
    "\tReturns:\n",
    "\t\t\tFAISS vector store object.\n",
    "\t\"\"\"\n",
    "\tindex_path = os.path.join(persist_directory, f'{collection_name}')\n",
    "\tif os.path.exists(index_path):\n",
    "\t\t# Load existing FAISS index\n",
    "\t\tprint(f\"Loading existing FAISS vector store from {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.load_local(\n",
    "\t\t\tindex_path, \n",
    "\t\t\tembeddings=EMBEDDING_FUNCTION, \n",
    "\t\t\tallow_dangerous_deserialization=True\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\t# Create new FAISS index\n",
    "\t\tprint(f\"Creating new FAISS vector store in {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.from_documents(\n",
    "\t\t\tdocuments, \n",
    "\t\t\tembedding=EMBEDDING_FUNCTION\n",
    "\t\t)\n",
    "\t\tfaiss_store.save_local(index_path)\n",
    "\treturn faiss_store\n",
    "\n",
    "def similarity_search(\n",
    "\tquestion,\n",
    "\tvector_store,\n",
    "\tk,\n",
    "\tdistance_threshold = 420.0\n",
    "):\n",
    "\t\"\"\"\n",
    "\tGet top k most similar documents using FAISS vector store.\n",
    "\tArgs:\n",
    "\t\tquestion: The user question\n",
    "\t\tvector_store: FAISS vector store\n",
    "\t\tk: Number of documents to return\n",
    "\t\tdistance_threshold: Maximum distance score to include document\n",
    "\tReturns:\n",
    "\t\tlist[Document]: Top k most similar documents\n",
    "\t\"\"\"\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "\treturn filtered_docs\n",
    "\n",
    "def get_hybrid_retriever(documents, vector_store, k):\n",
    "\t\"\"\"\n",
    "\tCreate a hybrid retriever combining BM25 and vector search.\n",
    "\tArgs:\n",
    "\t\tdocuments: List of documents for BM25 retriever.\n",
    "\t\tvector_store: FAISS vector store for vector retriever.\n",
    "\t\tk (int): Number of documents to retrieve.\n",
    "\tReturns:\n",
    "\t\tEnsembleRetriever object combining BM25 and vector search.\n",
    "\t\"\"\"\n",
    "\t# Create BM25 retriever\n",
    "\tbm25_retriever = BM25Retriever.from_documents(\n",
    "\t\tdocuments, \n",
    "\t\tk = 0\n",
    "\t)\n",
    "\t# Create vector retriever\n",
    "\tvector_retriever = vector_store.as_retriever(\n",
    "\t\tsearch_type=\"similarity\",\n",
    "\t\tsearch_kwargs={\n",
    "\t\t\t'k': k,\n",
    "\t\t}\n",
    "\t)\n",
    "\t# Combine retrievers with specified weights\n",
    "\tfusion_retriever = EnsembleRetriever(\n",
    "\t\tretrievers=[bm25_retriever, vector_retriever],\n",
    "\t\tweights=[0.2, 0.8]\n",
    "\t)\n",
    "\treturn fusion_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template\n",
    "\n",
    "- This code sets up a chatbot using LangChain that specifically answers questions about the SWEBOK textbook. The `system_prompt` defines guidelines for the chatbot, ensuring it identifies as a chatbot, restricts responses to provided context, avoids fabricating information, and maintains brevity.\n",
    "\n",
    "- The `ChatPromptTemplate` is created from the system instructions and a human input format, allowing for structured interactions. A function retrieves a brief description of the chatbot's purpose, confirming its focus on the SWEBOK content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "# Prompts\n",
    "system_prompt = \"\"\"  # Defines a multi-line string containing system instructions for the chatbot.\n",
    "You are a chatbot answering questions about \"Software Engineering: A Practitioner's Approach\" textbook.  # Specifies the chatbot's context and focus area.\n",
    "\n",
    "1. Always identify yourself as a chatbot, not the textbook.  # Instructs the chatbot to clarify its identity.\n",
    "2. Answer based only on provided context.  # Emphasizes using only the relevant context for responses.\n",
    "3. If unsure, say \"I don't have enough information to answer.\"  # Guides the chatbot on handling uncertainty in answers.\n",
    "4. For unclear questions, ask for clarification.  # Encourages the chatbot to seek more information for ambiguous questions.\n",
    "5. Keep responses under 256 tokens.  # Sets a limit on response length for conciseness.\n",
    "6. Don't invent information.  # Instructs the chatbot to refrain from generating unsupported information.\n",
    "7. Use context only if relevant.  # Advises the chatbot to incorporate context judiciously.\n",
    "8. To questions about your purpose, say: \"I'm a chatbot designed to answer questions about the 'Software Engineering: A Practitioner's Approach' textbook.\"  # Provides a standard response for inquiries about the chatbot's function.\n",
    "\n",
    "Be accurate and concise. Answer only what's asked.  # Reinforces the importance of precision and relevance in responses.\n",
    "\"\"\"\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([  # Creates a chat prompt template from the defined messages.\n",
    "    (\"system\", system_prompt),  # Sets the system prompt as the first message.\n",
    "    (\"human\", \"Question: {input}\\n\\nRelevant Context:\\n{context}\"),  # Defines the human user input format.\n",
    "])\n",
    "\n",
    "def get_chatbot_prompt_description():  # Defines a function that returns a description of the chatbot prompt.\n",
    "    return \"Chatbot prompt for answering textbook-related questions.\"  # Returns a brief description of the chatbot's purpose.\n",
    "\n",
    "# Calls the function to get the prompt description.\n",
    "output = get_chatbot_prompt_description()  \n",
    "print(output)  # Prints the description of the chatbot prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Document Loader\n",
    "\n",
    "- The provided code defines a function `load_documents_from_directory` that loads text documents from a specified directory. It iterates through all files in the given directory and checks if they have a `.txt` extension. For each valid text file, it uses the `TextLoader` from LangChain to load the document's content and appends it to a list.\n",
    "  \n",
    "-   Finally, the function returns a list of all loaded documents, allowing for easy access to the text data for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads text documents from a specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing text documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of loaded documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "            document = loader.load()\n",
    "            documents.extend(document)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Document Embeddings\n",
    "\n",
    "- The `load_embeddings` function retrieves document embeddings from a directory specified by the `CORPUS_SOURCE` environment variable. It checks for the variable's existence, loads the documents, and verifies successful loading. \n",
    "\n",
    "- It then calls `load_or_create_faiss_vector_store` to manage the FAISS vector store and `get_hybrid_retriever` to create a combined retriever. Finally, it confirms the loading process and returns the retriever for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    document_path = os.getenv(\"CORPUS_SOURCE\")\n",
    "    if not document_path:\n",
    "        raise ValueError(\"CORPUS_SOURCE not found in environment variables.\")\n",
    "    \n",
    "    persist_directory = os.path.join(document_path, \"faiss_indexes\")\n",
    "    top_k = 15\n",
    "    \n",
    "    # Load documents\n",
    "    documents = load_documents_from_directory(document_path)\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents loaded. Please check the document path.\")\n",
    "\n",
    "    # Assuming `load_or_create_faiss_vector_store` and `get_hybrid_retriever` are defined elsewhere\n",
    "    faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)\n",
    "    retriever = get_hybrid_retriever(documents, faiss_store, top_k)\n",
    "    \n",
    "    print(\"Embeddings and retriever loaded.\")\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral API Loader\n",
    "\n",
    "- The provided code defines a function `load_llm_api` that loads and configures the Mistral AI language model using an API key retrieved from environment variables. If the API key is not set, it raises an error. The function returns an instance of `ChatMistralAI`, configured with specified parameters like model name, temperature, maximum tokens, and top probability.\n",
    "  \n",
    "- The code then initializes the model with the name \"open-mistral-7b.\" A function named `chat_completion_as_dict` is included to interact with the model, taking a question as input and returning the model's response in a structured dictionary format, including the complete answer and the model name.\n",
    "\n",
    "- Retrieve an API key from an `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "def load_llm_api(model_name):\n",
    "    \"\"\"\n",
    "    Load and configure the Mistral AI LLM.\n",
    "    Returns:\n",
    "        ChatMistralAI: Configured LLM instance.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")  # Retrieve API key from environment variable\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is missing. Set the MISTRAL_API_KEY environment variable.\")\n",
    "    \n",
    "    return ChatMistralAI(\n",
    "        model=model_name,\n",
    "        mistral_api_key=api_key,\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.4,\n",
    "    )\n",
    "\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "llm = load_llm_api(MODEL_NAME)\n",
    "\n",
    "# Example function to test the LLM interaction\n",
    "def chat_completion_as_dict(question):\n",
    "    print(f\"Running prompt: {question}\")  # For debugging\n",
    "    response = llm.invoke([HumanMessage(content=question)])\n",
    "    return {\n",
    "        \"complete_answer\": response.content,\n",
    "        \"model\": MODEL_NAME\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
