{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot (Team 3)\n",
    "\n",
    "This chatbot serves as an educational resource designed to respond to questions about the textbook.\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)\n",
    "\n",
    "# Table of Contents\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Setup](#2.-Setup)\n",
    "3. [Building the Chatbot](#3.-Building-the-Chatbot)\n",
    "   - [Document loading](#3.1-Document-loading)\n",
    "   - [Embeddings](#3.2-Embeddings)\n",
    "   - [LLM setup](#3.3-LLM-setup)\n",
    "   - [Mistral loader](#3.4-Mistral-loader)\n",
    "4. [Improving the Chatbot with inference](#4.-Improving-the-Chatbot-with-inference)\n",
    "   - [Helpful functions](#4.1-Helpful-functions)\n",
    "   - [Prompt engineering](#4.2-Prompt-engineering)\n",
    "5. [Testing the Chatbot](#5.-Testing-the-Chatbot)\n",
    "6. [Conclusion](#6.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Purpose:\n",
    "This chatbot is as an educational tool that's built to answer questions related to the textbook, [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering). The chatbot was built by team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "Objective: \n",
    "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the SWEBOK textbook as the primary data source.\n",
    "\n",
    "Prerequisites:\n",
    "Github, Docker, Mamba, Python, Jupyter Notebook\n",
    "\n",
    "Resourses:\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To ensure compatibility, it is necessary to verify the Python version installed on your system. This project requires Python 3.10 or higher. Follow these steps to check and prepare your environment:\n",
    "\n",
    "Steps to Verify Python Version:\n",
    "\n",
    "- Check Installed Version:\n",
    "Open your terminal or command prompt and execute the following command:\n",
    "\n",
    "- For windows or Linux OS use command\n",
    "```python --version```\n",
    "\n",
    "- For Macos Os use command\n",
    "```!python3 --version```\n",
    "\n",
    "Dependency Requirements:\n",
    "- Python must already be installed on your system.\n",
    "- Python version of 3.10 or higher is mandatory for this project to function correctly.\n",
    "\n",
    "- If Python is not installed, download and install the latest version of Python from the official Python website.\n",
    "https://www.python.org/downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.0\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Virtual Environment\n",
    "This code is setting up a virtual environment for Python. Here's what it does in simple terms:\n",
    "\n",
    "- Install Required Tools:\n",
    "    - It makes sure necessary Python packages (`ipykernel` and `virtualenv`) are installed.\n",
    "    - These are tools needed for creating and managing the virtual environment.\n",
    "\n",
    "- Create a Virtual Environment:\n",
    "    - It creates a virtual environment named `myenv`.\n",
    "    - A virtual environment is like a separate workspace where you can install Python packages without affecting the global system settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.13 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!myenv/bin/pip install ipykernel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec myenv in /Users/kaushalnavapara/Library/Jupyter/kernels/myenv\n"
     ]
    }
   ],
   "source": [
    "!myenv/bin/python3 -m ipykernel install --user --name=myenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch Kernel and Verify:\n",
    "\n",
    "1. Switch the Kernel\n",
    "In the Jupyter Notebook interface, go to the menu bar and select Kernel > Change Kernel.\n",
    "A list of available kernels will appear. Choose the one that is appropriate for your environment (e.g., Python 3, or a specific virtual environment like myenv).\n",
    "- switch to ```myvenv```\n",
    "\n",
    "3. Verify the Kernel\n",
    "After switching the kernel, you can verify it by running the following commands in a new code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kaushalnavapara/Desktop/SE/csusb_fall2024_cse6550_team3/myenv/bin/python3.13\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update `pip` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "print(\"Updating pip\")\n",
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary dependencies\n",
    "\n",
    "- This cell installs essential packages for the chatbot and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies. This can take up to 3 minutes.\n",
      "Upgrading pip...\n",
      "Installing required libraries...\n",
      "\u001b[31mERROR: Cannot install sentence-transformers==0.1.0, sentence-transformers==0.2.0, sentence-transformers==0.2.1, sentence-transformers==0.2.2, sentence-transformers==0.2.3, sentence-transformers==0.2.4, sentence-transformers==0.2.4.1, sentence-transformers==0.2.5, sentence-transformers==0.2.5.1, sentence-transformers==0.2.6.1, sentence-transformers==0.2.6.2, sentence-transformers==0.3.0, sentence-transformers==0.3.1, sentence-transformers==0.3.2, sentence-transformers==0.3.3, sentence-transformers==0.3.4, sentence-transformers==0.3.5, sentence-transformers==0.3.5.1, sentence-transformers==0.3.6, sentence-transformers==0.3.7, sentence-transformers==0.3.7.1, sentence-transformers==0.3.7.2, sentence-transformers==0.3.8, sentence-transformers==0.3.9, sentence-transformers==0.4.0, sentence-transformers==0.4.1, sentence-transformers==0.4.1.1, sentence-transformers==0.4.1.2, sentence-transformers==1.0.0, sentence-transformers==1.0.1, sentence-transformers==1.0.2, sentence-transformers==1.0.3, sentence-transformers==1.0.4, sentence-transformers==1.1.0, sentence-transformers==1.1.1, sentence-transformers==1.2.0, sentence-transformers==1.2.1, sentence-transformers==2.0.0, sentence-transformers==2.1.0, sentence-transformers==2.2.0, sentence-transformers==2.2.1, sentence-transformers==2.2.2, sentence-transformers==2.3.0, sentence-transformers==2.3.1, sentence-transformers==2.4.0, sentence-transformers==2.5.0, sentence-transformers==2.5.1, sentence-transformers==2.6.0, sentence-transformers==2.6.1, sentence-transformers==2.7.0, sentence-transformers==3.0.0, sentence-transformers==3.0.1, sentence-transformers==3.1.0, sentence-transformers==3.1.1, sentence-transformers==3.2.0, sentence-transformers==3.2.1, sentence-transformers==3.3.0 and sentence-transformers==3.3.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0mDependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"\n",
    "    Installs required Python libraries using pip with the -q (quiet) flag.\n",
    "    \"\"\"\n",
    "    print(\"Installing dependencies. This can take up to 3 minutes.\")\n",
    "\n",
    "    # Upgrade pip\n",
    "    try:\n",
    "        print(\"Upgrading pip...\")\n",
    "        !{sys.executable} -m pip install --upgrade pip -q\n",
    "    except Exception as e:\n",
    "        print(f\"Error upgrading pip: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define required libraries\n",
    "    libraries = [\n",
    "        \"faiss-cpu\", \"huggingface_hub\", \"ipykernel\", \"jupyter\", \"langchain\",\n",
    "        \"langchain-community\", \"langchain-huggingface\", \"langchain-mistralai\",\n",
    "        \"python-dotenv\", \"sentence-transformers\", \"tiktoken\", \"pypdf\"\n",
    "    ]\n",
    "\n",
    "    # Install dependencies with -q flag for quiet output\n",
    "    try:\n",
    "        print(\"Installing required libraries...\")\n",
    "        libraries_str = \" \".join(libraries)\n",
    "        !{sys.executable} -m pip install -q {libraries_str}\n",
    "        print(\"Dependencies installed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during dependency installation: {e}\")\n",
    "\n",
    "# Call the function to install dependencies\n",
    "install_dependencies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Document loading\n",
    "\n",
    "Purpose: \n",
    "The code loads documents from a specified directory to build the data corpus for the chatbot\n",
    "\n",
    "Input:\n",
    "The input refers to the documents loaded from the directory specified by document_path, which will be used to process user queries related to the chatbot's knowledge base.\n",
    "\n",
    "Output:\n",
    "The output is the collection of documents loaded from the directory into the `documents` variable, which will be used for further processing in the chatbot.\n",
    "\n",
    "Processing:\n",
    "- The code loads documents from the specified directory into the `documents` variable, creating a corpus for the chatbot to use in responding to queries.\n",
    "- The primary data source used in this project is [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document. This can take up to 1 minute.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFDirectoryLoader\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Suppress warnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "print(\"Loading document. This can take up to 1 minute.\")\n",
    "# Importing required modules\n",
    "import os\n",
    "import sys\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adds the parent directory to the Python path for module imports\n",
    "sys.path.append(os.path.dirname(os.getcwd())) \n",
    "\n",
    "# Defines corpus source and paths for documents and FAISS indexes\n",
    "corpus_source = \"swebok\"  # Sets SWEBOK as corpus\n",
    "document_path = os.getcwd()\n",
    "persist_directory = os.path.join(document_path, \"faiss_indexes\")  # Directory for storing FAISS indexes\n",
    "\n",
    "def load_documents_from_directory(\n",
    "    document_path: str, \n",
    "    chunk_size: int = 2048, \n",
    "    chunk_overlap: int = 200\n",
    "):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a directory and split them into chunks.\n",
    "    \n",
    "    Args:\n",
    "        document_path (str): Path to the directory containing PDF files.\n",
    "        chunk_size (int): Size of each text chunk (default: 2048).\n",
    "        chunk_overlap (int): Overlap between chunks (default: 200).\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading documents from {document_path}...\")\n",
    "\n",
    "        # Check if the document path exists\n",
    "        if not os.path.exists(document_path):\n",
    "            raise FileNotFoundError(f\"Directory not found: {document_path}\")\n",
    "\n",
    "        # Load PDF documents from the specified directory\n",
    "        loader = PyPDFDirectoryLoader(document_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Create a text splitter using tiktoken encoder\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        return text_splitter.split_documents(documents)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load all documents from the defined directory\n",
    "documents = load_documents_from_directory(document_path)\n",
    "\n",
    "# Print the number of documents loaded\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Embeddings\n",
    "\n",
    "Purpose:\n",
    "Download and initialize the embedding model from HuggingFace to generate vector embeddings for text.\n",
    "\n",
    "Input:\n",
    "The input is the model name (`\"Alibaba-NLP/gte-large-en-v1.5\"`) which is used to fetch the embedding model from HuggingFace.\n",
    "\n",
    "Output:\n",
    "The output is the `EMBEDDING_FUNCTION`, which is an instance of the `HuggingFaceEmbeddings` class, ready to generate embeddings for text using the specified model.\n",
    "\n",
    "Processing:\n",
    "- we have retrieved the textbook, we need to create vector embeddings for it\n",
    "- We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as our vector database and [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) as our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating/loading the embeddings this will take a couple of minutes:\")\n",
    "# Download the embedding model from HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'trust_remote_code': True})\n",
    "print(\"Loaded embedded model successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Vector store\n",
    "Purpose:\n",
    "To create or load FAISS vector embeddings for the documents, allowing for efficient retrieval during chatbot interactions.\n",
    "\n",
    "Input:\n",
    "The input is the `documents` (loaded documents to be embedded) and `persist_directory` (the directory to store or retrieve the FAISS index).\n",
    "\n",
    "Output:\n",
    "The output is `faiss_store`, which is a FAISS vector store containing the document embeddings for efficient search and retrieval.\n",
    "\n",
    "Processing:\n",
    "The `load_or_create_faiss_vector_store` function processes the documents by either creating new FAISS embeddings or loading existing ones from the specified directory (`persist_directory`), enabling fast document retrieval based on vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Please wait... This step can take a long time when performed for the first time\")\n",
    "# Using pre-built load_or_create_faiss_vector_store function to create or load FAISS embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def load_or_create_faiss_vector_store(\n",
    "\tdocuments,\n",
    "\tpersist_directory,\n",
    "\tcollection_name=\"collection\"\n",
    "):\n",
    "\t\"\"\"\n",
    "\tLoad an existing FAISS vector store or create a new one if it doesn't exist.\n",
    "\tArgs:\n",
    "\t\t\tdocuments: List of documents to be indexed.\n",
    "\t\t\tcollection_name (str): Name of the collection.\n",
    "\t\t\tpersist_directory (str): Directory to save/load the FAISS index.\n",
    "\tReturns:\n",
    "\t\t\tFAISS vector store object.\n",
    "\t\"\"\"\n",
    "\tindex_path = os.path.join(persist_directory, f'{collection_name}')\n",
    "\tif os.path.exists(index_path):\n",
    "\t\t# Load existing FAISS index\n",
    "\t\tprint(f\"Loading existing FAISS vector store from {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.load_local(\n",
    "\t\t\tindex_path, \n",
    "\t\t\tembeddings=EMBEDDING_FUNCTION, \n",
    "\t\t\tallow_dangerous_deserialization=True\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\t# Create new FAISS index\n",
    "\t\tprint(f\"Creating new FAISS vector store in {index_path}...\\n\")\n",
    "\t\tfaiss_store = FAISS.from_documents(\n",
    "\t\t\tdocuments, \n",
    "\t\t\tembedding=EMBEDDING_FUNCTION\n",
    "\t\t)\n",
    "\t\tfaiss_store.save_local(index_path)\n",
    "\treturn faiss_store\n",
    "\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 LLM setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "To load environment variables from a `.env` file, retrieve the Mistral API key, and ensure the key is available for further usage in the application.\n",
    "\n",
    "Input:\n",
    "- `.env` file containing environment variables (like `MISTRAL_API_KEY`).\n",
    "- `api_key`: A string variable that may hold the Mistral API key (if manually provided).\n",
    "\n",
    "Output:\n",
    "Prints \"Environment variables successfully setup\" if successful, or raises an error if the `MISTRAL_API_KEY` is not found.\n",
    "\n",
    "Processing:\n",
    "we have to setup environment variables that will contain our API keys.\n",
    "- If you have already created a `.env` file and added the `MISTRAL_API_KEY` you do not have to do anything. \n",
    "- If not, then you can add your API key below. Get an API key [here](https://console.mistral.ai/api-keys/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = \"\" # add your Mistral API key here if needed\n",
    "if api_key == \"\":\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "elif not api_key:\n",
    "\traise ValueError(\"MISTRAL API KEY not found\")\n",
    "print(\"Environment variables succesfully setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Mistral loader\n",
    "\n",
    "Purpose:\n",
    "To load the Mistral AI model (in this case, \"open-mistral-7b\") using the `ChatMistralAI` class from `langchain_mistralai`, and configure it with necessary parameters (such as temperature, max tokens, and top-p) for generating responses.\n",
    "\n",
    "Input:\n",
    "- `model_name`: The name of the pre-trained model to be used, here set as \"open-mistral-7b\".\n",
    "- `api_key`: The Mistral API key used for authenticating the model access.\n",
    "\n",
    "Output:\n",
    "- The model is loaded and ready to be used for generating responses.\n",
    "- Prints \"Successfully loaded Mistral 7B\" upon successful loading of the model.\n",
    "\n",
    "Processing:\n",
    "We will be using [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) as our primary large language model. This will combined with our retriever to create our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load and configure the Mistral AI LLM.\n",
    "model_name = \"open-mistral-7b\"\n",
    "def load_llm(model_name):\n",
    "    return ChatMistralAI(\n",
    "        model=model_name,  # Model name\n",
    "        mistral_api_key=api_key,  # Mistral API key\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.4,\n",
    "    )\n",
    "\n",
    "llm = load_llm(model_name)\n",
    "print(\"Successfully loaded Mistral 7B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving the Chatbot with inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "Retrieve and filter the top k most similar documents from the FAISS vector store based on a question.\n",
    "\n",
    "Input:\n",
    "- `question`: The user's query.\n",
    "- `vector_store`: FAISS vector store.\n",
    "- `k`: Number of similar documents to return.\n",
    "- `distance_threshold`: Score threshold for filtering.\n",
    "\n",
    "Output:\n",
    "Filtered list of documents that are most similar to the question.\n",
    "\n",
    "Processing:\n",
    "Perform a similarity search in the vector store, filter documents based on the score threshold, return the relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k most similar documents using FAISS vector store.\n",
    "def similarity_search(question, vector_store, k, distance_threshold = 420.0):\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k)\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold]\n",
    "\treturn filtered_docs\n",
    "print(\"Performs document similarity search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-based response generation\n",
    "Purpose:\n",
    "Generate responses to a user's question using the RAG system by combining relevant documents and the LLM\n",
    "\n",
    "Input:\n",
    "- `question`: The user's query\n",
    "- `prompt`: The system's prompt format\n",
    "- `llm`: The language model to generate the response\n",
    "\n",
    "Output:\n",
    "Streamed response chunks as an answer, enriched with context from the relevant documents\n",
    "\n",
    "Processing:\n",
    "Retrieve the top k relevant documents using `similarity_search`, format the context from the documents and the user query, use the LLM to generate a response, streaming chunks of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the RAG system to answer the user's questions\n",
    "def chat_completion(question, prompt, llm):\n",
    "    top_k = 10 # The maximum number of documents that similarity search will return\n",
    "    \n",
    "    relevant_docs = similarity_search(question, faiss_store, top_k) # Get relevant documents\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs]) # Format retrived documents\n",
    "    messages = prompt.format_messages(input=question, context=context) \n",
    "    \n",
    "    # Stream response\n",
    "    full_response = {\"answer\": \"\", \"context\": relevant_docs}\n",
    "    for chunk in llm.stream(messages):\n",
    "        full_response[\"answer\"] += chunk.content\n",
    "        yield (chunk.content)\n",
    "print(\"Performs chat completion function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive chatbot interface\n",
    "Purpose:\n",
    "Provide an interactive interface where the user can input a question (or prompt) and get a response from the chatbot by invoking the RAG system\n",
    "\n",
    "Input:\n",
    "User-provided prompt (query) through the `prompt_input` widget\n",
    "\n",
    "Output:\n",
    "Display the chatbot's response, streamed in chunks, in the output widget\n",
    "\n",
    "Processing:\n",
    "- Create Input/Output Widgets: `prompt_input` for user input, `submit_button` for triggering the action, and `output` to display the response\n",
    "- Button Click Action: When the button is clicked, it triggers the `on_submit` function\n",
    "- Response Generation: The function uses the `chat_completion` to generate a response based on the user query. It then streams and displays the response in the `output` widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Prompt widget\n",
    "prompt_input = widgets.Text(\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "# Sumbit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "output = widgets.Output()\n",
    "def on_submit(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        user_prompt = prompt_input.value\n",
    "        if not user_prompt:\n",
    "            user_prompt = \"Who is Hironori Washizaki?\"\n",
    "        print(f\"\\nPrompt: {user_prompt}\\n\")\n",
    "        # Stream the response\n",
    "        for response_chunk in chat_completion(user_prompt, prompt, llm):\n",
    "            print(response_chunk, end='', flush=True)\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "print(\"Building interactive chatbot response system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prompt engineering\n",
    "Purpose:\n",
    "Define the system behavior for the chatbot and format the prompt template for interacting with the user and providing responses based on the context.\n",
    "\n",
    "Input:\n",
    "The `system_prompt` specifies the chatbot’s instructions for how to respond.\n",
    "The prompt template defines how the question and context are formatted for the chatbot.\n",
    "\n",
    "Output:\n",
    "A formatted prompt template (`prompt`) that combines the system instructions and user input (question and context) for processing by the LLM\n",
    "\n",
    "Processing:\n",
    "- System Behavior Definition: The `system_prompt` sets rules for how the chatbot should answer questions, handle uncertainty, and identify itself\n",
    "- Prompt Template Creation: The `ChatPromptTemplate` is created with a combination of the system prompt and the user’s question/context format, ready to be used for generating the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The system prompt will be used as a framework drive the LLM responses\n",
    "system_prompt = \"\"\"\n",
    "You are a chatbot that answers the question in the <question> tags.\n",
    "- Answer based only on provided context in <context> tags only if relevant.\n",
    "- If unsure, say \"I don't have enough information to answer.\"\n",
    "- For unclear questions, ask for clarification.\n",
    "- Always identify yourself as a chatbot, not the textbook.\n",
    "- To questions about your purpose, say: \"I'm a chatbot designed to answer questions about the provided textbook.\"\n",
    "\"\"\"\n",
    "\n",
    "# Setting up a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system_prompt),\n",
    "  (\"human\", \"<question>{input}</question>\\n\\n<context>{context}<context>\"),\n",
    "])\n",
    "print(\"Generates system prompt and template.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing the Chatbot\n",
    "\n",
    "Purpose:\n",
    "Render interactive widgets for user input, a submit button, and output display in the notebook\n",
    "\n",
    "Input:\n",
    "- `prompt_input`: User's query\n",
    "- `submit_button`: Button to trigger response generation\n",
    "- `output`: Area to display the response\n",
    "\n",
    "Output:\n",
    "Displays input field, submit button, and output area for chatbot interaction.\n",
    "\n",
    "Processing:\n",
    "- User enters a query and clicks the button\n",
    "- The chatbot processes the query and displays the response in the output area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(prompt_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Recap:\n",
    "- Developed a chatbot using the RAG system, which retrieves relevant documents and generates responses based on the context provided\n",
    "- Integrated widgets in Jupyter Notebook for interactive user input and response display\n",
    "- Configured the chatbot with natural language processing models like Mistral AI and vector-based document retrieval using FAISS\n",
    "\n",
    "Next Steps:\n",
    " Scale the chatbot's knowledge base to handle a larger variety of questions or integrate multiple datasets from other sources \n",
    "\n",
    "Resourses:\n",
    "- The Textbook Chatbot project was built by Team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/) offered at CSUSB\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
