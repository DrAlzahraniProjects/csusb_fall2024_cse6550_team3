{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Chatbot (Team 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Setup](#2.-Setup)\n",
    "3. [Building the Chatbot](#3.-Building-the-Chatbot)\n",
    "   - [Document loading](#3.1-Document-loading)\n",
    "   - [Embeddings](#3.2-Embeddings)\n",
    "   - [LLM setup](#3.3-LLM-setup)\n",
    "   - [Mistral loader](#3.4-Mistral-loader)\n",
    "4. [Improving the Chatbot with inference](#4.-Improving-the-Chatbot-with-inference)\n",
    "   - [Helpful functions](#4.1-Helpful-functions)\n",
    "   - [Prompt engineering](#4.2-Prompt-engineering)\n",
    "5. [Testing the Chatbot](#5.-Testing-the-Chatbot)\n",
    "6. [Conclusion](#6.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Purpose:\n",
    "This chatbot is as an educational tool that's built to answer questions related to the textbook, [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering). The chatbot was built by team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "Objective: \n",
    "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the SWEBOK textbook as the primary data source.\n",
    "\n",
    "Prerequisites:\n",
    "Github, Docker, Mamba, Python, Jupyter Notebook\n",
    "\n",
    "Resourses:\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: \n",
    "Set up the necessary tools and libraries for building a retrieval-augmented chatbot that processes documents, creates embeddings, and retrieves relevant information efficiently.\n",
    "\n",
    "Input: \n",
    "Environment variables (e.g., API keys) loaded using `load_dotenv`\n",
    "PDFs or text documents to be processed\n",
    "User queries for retrieval\n",
    "\n",
    "Output:\n",
    "Configures tools for document loading, text splitting, embedding generation, retrieval (FAISS, BM25, Ensemble), and response generation chains\n",
    "\n",
    "Processing:\n",
    "- Environment: Load `.env` for API keys/settings\n",
    "- Document Loading: Use `PyPDFDirectoryLoader` for PDFs\n",
    "- Text Splitting: Split documents with `RecursiveCharacterTextSplitter`\n",
    "- Embeddings: Generate vectors via Hugging Face models\n",
    "- Retrievers: Use FAISS, BM25, and EnsembleRetriever for accuracy\n",
    "- Response: Build retrieval pipelines with `create_retrieval_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain # import query handling through retrieval-based chains.\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # import combining documents in retrieval chains\n",
    "from dotenv import load_dotenv # Loads environment variables from a .env file.\n",
    "from langchain_community.vectorstores import FAISS # Imports FAISS for vector store management.\n",
    "from langchain_community.retrievers import BM25Retriever # Imports BM25 retriever for document retrieval.\n",
    "from langchain.retrievers import EnsembleRetriever # Imports Ensemble retriever for combining multiple retrieval strategies.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # For splitting text into smaller chunks.\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # For loading documents from a directory of PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Document loading\n",
    "\n",
    "Purpose: \n",
    "The code loads documents from a specified directory to build the data corpus for the chatbot\n",
    "\n",
    "Input:\n",
    "The input refers to the documents loaded from the directory specified by document_path, which will be used to process user queries related to the chatbot's knowledge base.\n",
    "\n",
    "Output:\n",
    "The output is the collection of documents loaded from the directory into the `documents` variable, which will be used for further processing in the chatbot.\n",
    "\n",
    "Processing:\n",
    "- The code loads documents from the specified directory into the `documents` variable, creating a corpus for the chatbot to use in responding to queries.\n",
    "- The primary data source used in this project is [Software Engineering Body of Knowledge (SWEBOK)](https://www.computer.org/education/bodies-of-knowledge/software-engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from /app/data/swebok...\n"
     ]
    }
   ],
   "source": [
    "# Importing OS module for interacting with the operating system\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Adds the parent directory to the Python path for module imports.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "corpus_source = \"swebok\" # Sets swebok as corpus.\n",
    "\n",
    "# Defines the path to the directory containing the documents.\n",
    "document_path = os.path.abspath(os.path.join(\"../data\", corpus_source)) \n",
    "persist_directory = os.path.join(document_path, \"faiss_indexes\") # Specifies the directory for storing FAISS indexes.\n",
    "\n",
    "# Imports a custom function to load documents from a specified directory.\n",
    "from backend.document_loading import load_documents_from_directory \n",
    "# Loads all documents from the defined directory.\n",
    "documents = load_documents_from_directory(document_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Embeddings\n",
    "\n",
    "Purpose:\n",
    "Download and initialize the embedding model from HuggingFace to generate vector embeddings for text.\n",
    "\n",
    "Input:\n",
    "The input is the model name (`\"Alibaba-NLP/gte-large-en-v1.5\"`) which is used to fetch the embedding model from HuggingFace.\n",
    "\n",
    "Output:\n",
    "The output is the `EMBEDDING_FUNCTION`, which is an instance of the `HuggingFaceEmbeddings` class, ready to generate embeddings for text using the specified model.\n",
    "\n",
    "Processing:\n",
    "- we have retrieved the textbook, we need to create vector embeddings for it\n",
    "- We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) as our vector database and [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) as our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embedding model from HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\" # Specifies the Alibaba embedding model.\n",
    "EMBEDDING_FUNCTION = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'trust_remote_code': True}) # Initializes embedding function with the selected model and settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "To create or load FAISS vector embeddings for the documents, allowing for efficient retrieval during chatbot interactions.\n",
    "\n",
    "Input:\n",
    "The input is the `documents` (loaded documents to be embedded) and `persist_directory` (the directory to store or retrieve the FAISS index).\n",
    "\n",
    "Output:\n",
    "The output is `faiss_store`, which is a FAISS vector store containing the document embeddings for efficient search and retrieval.\n",
    "\n",
    "Processing:\n",
    "The `load_or_create_faiss_vector_store` function processes the documents by either creating new FAISS embeddings or loading existing ones from the specified directory (`persist_directory`), enabling fast document retrieval based on vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS vector store from /app/data/swebok/faiss_indexes/collection...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pre-built load_or_create_faiss_vector_store function to create or load FAISS embeddings\n",
    "from backend.document_loading import load_or_create_faiss_vector_store\n",
    "faiss_store = load_or_create_faiss_vector_store(documents, persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 LLM setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "To load environment variables from a `.env` file, retrieve the Mistral API key, and ensure the key is available for further usage in the application.\n",
    "\n",
    "Input:\n",
    "- `.env` file containing environment variables (like `MISTRAL_API_KEY`).\n",
    "- `api_key`: A string variable that may hold the Mistral API key (if manually provided).\n",
    "\n",
    "Output:\n",
    "Prints \"Environment variables successfully setup\" if successful, or raises an error if the `MISTRAL_API_KEY` is not found.\n",
    "\n",
    "Processing:\n",
    "we have to setup environment variables that will contain our API keys.\n",
    "- If you have already created a `.env` file and added the `MISTRAL_API_KEY` you do not have to do anything. \n",
    "- If not, then you can add your API key below. Get an API key [here](https://console.mistral.ai/api-keys/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables succesfully setup\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = \"\" # Initializes the API key variable.\n",
    "if api_key == \"\":\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\") # Retrieves the API key from environment variables if not already set.\n",
    "elif not api_key:\n",
    "\traise ValueError(\"MISTRAL API KEY not found\") # Raises an error if the API key is not found.\n",
    "print(\"Environment variables succesfully setup\") # Confirms the environment setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Mistral loader\n",
    "\n",
    "Purpose:\n",
    "To load the Mistral AI model (in this case, \"open-mistral-7b\") using the `ChatMistralAI` class from `langchain_mistralai`, and configure it with necessary parameters (such as temperature, max tokens, and top-p) for generating responses.\n",
    "\n",
    "Input:\n",
    "- `model_name`: The name of the pre-trained model to be used, here set as \"open-mistral-7b\".\n",
    "- `api_key`: The Mistral API key used for authenticating the model access.\n",
    "\n",
    "Output:\n",
    "- The model is loaded and ready to be used for generating responses.\n",
    "- Prints \"Successfully loaded Mistral 7B\" upon successful loading of the model.\n",
    "\n",
    "Processing:\n",
    "We will be using [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) as our primary large language model. This will combined with our retriever to create our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded Mistral 7B\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "model_name = \"open-mistral-7b\" # Specifies the model open Mistral-7b AI chat model.\n",
    "# Defines a function to initialize the Mistral AI chat model with specific parameters.\n",
    "def load_llm(model_name):\n",
    "\treturn ChatMistralAI(\n",
    "\t\tmodel=model_name,\n",
    "\t\tmistral_api_key=api_key,\n",
    "\t\ttemperature=0.2,\n",
    "\t\tmax_tokens=256,\n",
    "\t\ttop_p=0.4,\n",
    "\t)\n",
    "    \n",
    "llm = load_llm(model_name) # Loads the Mistral AI chat model using the specified parameters.\n",
    "print(\"Succesfully loaded Mistral 7B\") # Confirms that the model was loaded successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving the Chatbot with inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "Retrieve and filter the top k most similar documents from the FAISS vector store based on a question.\n",
    "\n",
    "Input:\n",
    "- `question`: The user's query.\n",
    "- `vector_store`: FAISS vector store.\n",
    "- `k`: Number of similar documents to return.\n",
    "- `distance_threshold`: Score threshold for filtering.\n",
    "\n",
    "Output:\n",
    "Filtered list of documents that are most similar to the question.\n",
    "\n",
    "Processing:\n",
    "Perform a similarity search in the vector store, filter documents based on the score threshold, return the relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k most similar documents using FAISS vector store.\n",
    "def similarity_search(question, vector_store, k, distance_threshold = 420.0):\n",
    "\tretrieved_docs = vector_store.similarity_search_with_score(question, k=k) # Filters documents by score threshold.\n",
    "\tfiltered_docs = [doc for doc, score in retrieved_docs if score <= distance_threshold] # Returns the filtered documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "Generate responses to a user's question using the RAG system by combining relevant documents and the LLM\n",
    "\n",
    "Input:\n",
    "- `question`: The user's query\n",
    "- `prompt`: The system's prompt format\n",
    "- `llm`: The language model to generate the response\n",
    "\n",
    "Output:\n",
    "Streamed response chunks as an answer, enriched with context from the relevant documents\n",
    "\n",
    "Processing:\n",
    "Retrieve the top k relevant documents using `similarity_search`, format the context from the documents and the user query, use the LLM to generate a response, streaming chunks of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the RAG system to answer the user's questions\n",
    "def chat_completion(question, prompt, llm):\n",
    "    top_k = 10   # Sets the number of top relevant documents to retrieve.\n",
    "    \n",
    "    relevant_docs = similarity_search(question, faiss_store, top_k)  # Retrieves top `k` relevant documents.\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])  # Combines retrieved documents into context.\n",
    "    messages = prompt.format_messages(input=question, context=context)  # Prepares input and context for the model.\n",
    "    full_response = {\"answer\": \"\", \"context\": relevant_docs}\n",
    "    for chunk in llm.stream(messages): # Streams the response chunks from the language model.\n",
    "        full_response[\"answer\"] += chunk.content\n",
    "        yield (chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "Provide an interactive interface where the user can input a question (or prompt) and get a response from the chatbot by invoking the RAG system\n",
    "\n",
    "Input:\n",
    "User-provided prompt (query) through the `prompt_input` widget\n",
    "\n",
    "Output:\n",
    "Display the chatbot's response, streamed in chunks, in the output widget\n",
    "\n",
    "Processing:\n",
    "- Create Input/Output Widgets: `prompt_input` for user input, `submit_button` for triggering the action, and `output` to display the response\n",
    "- Button Click Action: When the button is clicked, it triggers the `on_submit` function\n",
    "- Response Generation: The function uses the `chat_completion` to generate a response based on the user query. It then streams and displays the response in the `output` widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Creates a text input widget for the user to enter a prompt.\n",
    "prompt_input = widgets.Text(\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "# Creates a button widget labeled 'Submit' with a primary style.\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "output = widgets.Output() # Creates an output widget to display the results.\n",
    "def on_submit(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        user_prompt = prompt_input.value  # Retrieves the user's input from the text widget.\n",
    "        if not user_prompt:\n",
    "            user_prompt = \"Who is Hironori Washizaki?\" # Sets a default question if no prompt is provided.\n",
    "        print(f\"\\nPrompt: {user_prompt}\\n\")  # Displays the user's prompt.\n",
    "        for response_chunk in chat_completion(user_prompt, prompt, llm): # Streams the chatbot's response chunks.\n",
    "            print(response_chunk, end='', flush=True)\n",
    "# Binds the button click event to the `on_submit` function.\n",
    "submit_button.on_click(on_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prompt engineering\n",
    "Purpose:\n",
    "Define the system behavior for the chatbot and format the prompt template for interacting with the user and providing responses based on the context.\n",
    "\n",
    "Input:\n",
    "The `system_prompt` specifies the chatbot’s instructions for how to respond.\n",
    "The prompt template defines how the question and context are formatted for the chatbot.\n",
    "\n",
    "Output:\n",
    "A formatted prompt template (`prompt`) that combines the system instructions and user input (question and context) for processing by the LLM\n",
    "\n",
    "Processing:\n",
    "- System Behavior Definition: The `system_prompt` sets rules for how the chatbot should answer questions, handle uncertainty, and identify itself\n",
    "- Prompt Template Creation: The `ChatPromptTemplate` is created with a combination of the system prompt and the user’s question/context format, ready to be used for generating the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Defines a system prompt that instructs the chatbot on how to answer questions.\n",
    "system_prompt = \"\"\"\n",
    "You are a chatbot that answers the question in the <question> tags.\n",
    "- Answer based only on provided context in <context> tags only if relevant.\n",
    "- If unsure, say \"I don't have enough information to answer.\"\n",
    "- For unclear questions, ask for clarification.\n",
    "- Always identify yourself as a chatbot, not the textbook.\n",
    "- To questions about your purpose, say: \"I'm a chatbot designed to answer questions about the provided textbook.\"\n",
    "\"\"\"\n",
    "\n",
    "# Creates a prompt template using the system prompt and human input format.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system_prompt),\n",
    "  (\"human\", \"<question>{input}</question>\\n\\n<context>{context}<context>\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "Render interactive widgets for user input, a submit button, and output display in the notebook\n",
    "\n",
    "Input:\n",
    "- `prompt_input`: User's query\n",
    "- `submit_button`: Button to trigger response generation\n",
    "- `output`: Area to display the response\n",
    "\n",
    "Output:\n",
    "Displays input field, submit button, and output area for chatbot interaction.\n",
    "\n",
    "Processing:\n",
    "- User enters a query and clicks the button\n",
    "- The chatbot processes the query and displays the response in the output area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb53606ea22403bb34eff11c1e5cd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', layout=Layout(width='500px'), placeholder='Enter your prompt here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b805ee1d564104bbf801c4937b5a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9379eb69ea0c46b482d44e04a3a7d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': '\\nPrompt: Who is Hironori Washizaki?\\n\\nHironori Washizaki is an i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prompt_input, submit_button, output) # Displays the text input widget, submit button, and output widget for user interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Recap:\n",
    "- Developed a chatbot using the RAG system, which retrieves relevant documents and generates responses based on the context provided\n",
    "- Integrated widgets in Jupyter Notebook for interactive user input and response display\n",
    "- Configured the chatbot with natural language processing models like Mistral AI and vector-based document retrieval using FAISS\n",
    "\n",
    "Next Steps:\n",
    " Scale the chatbot's knowledge base to handle a larger variety of questions or integrate multiple datasets from other sources \n",
    "\n",
    "Resourses:\n",
    "- The Textbook Chatbot project was built by Team 3 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/) offered at CSUSB\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team3/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
